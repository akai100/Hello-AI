
torch.cuda用于设置和运行CUDA操作。它会跟踪当前选中的GPU，你分配的所有CUDA张量默认都会在该设备上创建。可以使用torch.cuda.device上下文管理器更改选中的设备。

然而，一旦张量被分配，你就可以在其上执行操作，而无需考虑所选设备，并且结果将始终与该张量位于同一设备上。

默认情况下不允许跨GPU操作，但copy_()以及其他具有类似复制功能的方法（如to()和cuda()）除外。除非启用对等内存访问，否则任何尝试在分布于不同设备上的张量上运行操作都会引发错误。

# 1. Ampere（及后续）设备上的TensorFloat-32（TF32）

在PyTorch 2.9之后，我们提供了一套新的API，以更精细的方式控制TF32的行为，并建议使用这些新API以获得更好的控制效果。我们可以为每个后端和每个算子设置float32精度。我们还可以为特定算子覆盖全局设置。

    TF32 是一种 “折中精度”（NVIDIA 推出的混合精度格式），兼顾 float32 的精度和 float16 的速度，常用于 GPU 上的
    矩阵乘法（matmul）、卷积（conv）等计算密集型算子，旧版 PyTorch 对 TF32 的控制多是 “全局开关”（比如全局启用 / 禁用），不够灵活。
    2. 新 API 的核心改进点
    （1）控制粒度更细：按 “后端 + 算子” 拆分
    按后端：支持为不同计算后端（比如 CUDA、CPU，核心是 GPU 后端）单独设置 TF32 行为（比如仅让 CUDA 后端的算子使用 TF32，CPU 保持纯 float32）；
    按算子：不再是 “所有算子统一规则”，而是可以针对单个算子（比如 torch.matmul、torch.nn.Conv2d）单独配置 float32/TF32 精度（
    比如让卷积用 TF32 提速，矩阵乘法保持纯 float32 保精度）。
    （2）支持 “全局默认 + 局部覆盖”
    先设置全局规则（比如默认所有 CUDA 算子启用 TF32）；
    对需要特殊处理的算子，单独设置局部规则覆盖全局（比如全局启用 TF32，但某个关键的 matmul 算子必须用纯 float32，直接为该算子配置即可）。
    3. 核心价值
    平衡 “速度” 和 “精度”：无需为了全局提速牺牲关键算子的精度，也无需为了保精度放弃非关键算子的性能收益；
    适配复杂场景：比如混合精度训练中，部分算子对精度敏感（如损失计算、梯度聚合），部分算子（如特征提取的卷积）可容忍 TF32，新 API 
    能精准匹配这种需求。
