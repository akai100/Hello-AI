# 1. 使用```torch.autograd```进行自动微分

训练神经网络时，最常用的算法是反向传播。在该算法中，参数（模型权重）会根据损失函数相对于给定参数的梯度进行调整。

为了计算这些梯度，PyTorch 有一个内置的微分引擎，称为 ```torch.autograd```。它支持为任何计算图自动计算梯度。

考虑最简单的单层神经网络，它具有输入x、参数w和b，以及某个损失函数。在PyTorch中可以按以下方式定义它:

```python3
import torch

x = torch.ones(5)  # input tensor
y = torch.zeros(3)  # expected output
w = torch.randn(5, 3, requires_grad=True)
b = torch.randn(3, requires_grad=True)
z = torch.matmul(x, w)+b
loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)

```

# 2. 张量、函数和计算图

在这个网络中，```w```和```b```是参数，我们需要对其进行优化。因此，我们需要能够计算损失函数相对于这些变量的梯度。为了做到这一点，我们需要设置这些张量的```requires_grad```属性。

我们应用于张量以构建计算图的函数，实际上是Function类的一个对象。这个对象知道如何在前向方向计算函数，也知道如何在反向传播步骤中计算其导数。反向传播函数的引用存储在张量的```grad_fn```属性中。

```python3
print(f"Gradient function for z = {z.grad_fn}")
print(f"Gradient function for loss = {loss.grad_fn}")
```

# 3. 计算梯度

# 4. 禁用梯度跟踪



