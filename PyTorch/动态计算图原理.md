PyTorch 采用“边计算、边建图”的动态模式。

# 1. 核心概念

计算图是描述张量运算流程的有向无环图（DAG），核心由两部分组成：

+ 节点

  分为两类，数据节点（张量 Tensor）和运算节点；

+ 边

  表示数据流向与依赖关系，既承载张量的数值传递，也记录反向传播时的梯度传播路径；

# 2. 工作流程：前向建图 + 反向求导

1. 前向传播：构建计算图 + 计算数值

+ 数值计算：得到运算结果；

+ 图结构构建

  自动记录输入张量 -> 运算 -> 输出张量的依赖关系，同时为每个输出张量绑定 grd_fn 属性（梯度函数），标记该张量由哪个运算生成，为反向传播做准备。

# 3， 核心机制：梯度追踪与计算图控制

1. ```requires_grad```：梯度追踪开关

控制张量是否参与梯度计算，仅```requires_grad=True```的张量会被纳入计算图，其梯度会被记录。

2. ```detach```: 断开计算图连接

返回一个与原张量数据共享，但 requires_grad=False 的副本，断开该副本与原计算图的依赖关系。

3. ```torch.no_grad()```：临时禁用梯度追踪

上下文管理器，临时禁用代码块内的所有梯度追踪，不改变张量本身的 requires_grad 属性。

```python3
with torch.no_grad():
    ...
```

4. 自定义梯度

当 LLM 需实现自定义算子（如特殊激活函数、高效注意力变体）时，可通过继承此类，手动定义前向计算和梯度传播逻辑，兼顾灵活性与效率。

```python3
class SwishFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x):
        ctx.save_for_backward(x)  # 保存前向张量，供反向传播使用
        return x * torch.sigmoid(x)  # Swish(x) = x * sigmoid(x)
    
    @staticmethod
    def backward(ctx, grad_output):
        x = ctx.saved_tensors[0]
        sigmoid_x = torch.sigmoid(x)
        # 手动推导梯度：dSwish/dx = sigmoid(x) + x*sigmoid(x)*(1-sigmoid(x))
        grad_x = grad_output * (sigmoid_x + x * sigmoid_x * (1 - sigmoid_x))
        return grad_x

# LLM 中调用自定义激活函数
swish = SwishFunction.apply
y = swish(x @ w + b)  # 自动支持梯度传播
```



