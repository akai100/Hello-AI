
```torch.use_deterministic_algorithms()``` 允许你配置 PyTorch，使其在可用的情况下使用确定性算法而非非确定性算法，
并且如果某个操作已知为非确定性的（且没有确定性替代方案），会抛出错误。

## 1. CUDA卷积确定性

虽然禁用CUDA卷积基准测试（上文已讨论）可确保CUDA在每次运行应用程序时选择相同的算法，但该算法本身可能具有非确定性，
除非设置了```torch.use_deterministic_algorithms(True)```或```torch.backends.cudnn.deterministic = True```。
后者的设置仅控制此行为，而```torch.use_deterministic_algorithms()```还会使其他PyTorch操作也具有确定性。


## 2. CUDA RNN 和 LSTM

在某些版本的CUDA中，循环神经网络（RNN）和长短期记忆网络（LSTM）可能具有非确定性行为。有关详细信息和解决方法，请参阅```torch.nn.RNN()```和```torch.nn.LSTM()```。

## 3. 填充未初始化的内存

像```torch.empty()```和```torch.Tensor.resize_()```这类操作，可能会返回包含未初始化内存的张量，这些内存中含有未定义的值。
如果需要确定性，将这样的张量用作另一操作的输入是无效的，因为输出会具有不确定性。但实际上并没有什么能阻止这类无效代码的运行。
因此，为了安全起见，```torch.utils.deterministic.fill_uninitialized_memory```默认被设置为True，
如果设置了```torch.use_deterministic_algorithms(True)```，它会用一个已知值填充未初始化的内存。这将避免此类不确定性行为的发生。

然而，填充未初始化的内存会对性能产生不利影响。因此，如果你的程序是有效的，并且不会将未初始化的内存用作某个操作的输入，那么可以关闭此设置以获得更好的性能。
