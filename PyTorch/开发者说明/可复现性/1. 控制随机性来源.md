## 1. PyTorch随机数生成器

你可以使用```torch.manual_seed()```为所有设备（包括CPU和CUDA）的随机数生成器设置种子：

```python3
import torch
torch.manual_seed(0)
```

一些PyTorch操作可能会在内部使用随机数。例如，```torch.svd_lowrank()```就是如此。
因此，使用相同的输入参数连续多次调用它可能会得到不同的结果。但是，只要在应用程序开始时将```torch.manual_seed()```设置为一个常数，
并且消除了所有其他的非确定性来源，那么每次在相同环境中运行该应用程序时，都会生成相同的随机数序列。

通过在后续调用之间将```torch.manual_seed()```设置为相同的值，也可以从使用随机数的操作中获得相同的结果。

## 2. Python

对于自定义运算符，您可能还需要设置Python种子：

```python3
import random
random.seed(0)
```

## 3. 其他库中的随机数生成器

如果你或你正在使用的任何库依赖于NumPy，你可以通过以下方式为全局NumPy随机数生成器设置种子：

```python3
import numpy as np
np.random.seed(0)
```

然而，一些应用程序和库可能会使用NumPy随机生成器对象，而非全局随机数生成器,这些对象也需要被一致地设置种子。

如果你正在使用任何其他使用随机数生成器的库，请参考这些库的文档，了解如何为它们设置一致的种子。

## 4. CUDA 卷积基准测试

CUDA卷积运算所使用的cuDNN库，可能是导致应用程序多次执行时出现非确定性的一个原因。
当调用cuDNN卷积时使用一组新的尺寸参数，一项可选功能会运行多种卷积算法，并对它们进行基准测试以找出最快的一种。
之后，在该进程的剩余时间里，对于相应的这组尺寸参数，会始终使用这种最快的算法。
由于基准测试中的干扰以及硬件的差异，即使在同一台机器上，后续运行时基准测试可能会选择不同的算法。

使用```torch.backends.cudnn.benchmark = False```禁用基准测试功能会导致cuDNN确定性地选择一种算法，这可能会以降低性能为代价。

但是，如果你的应用程序不需要在多次执行中保持可重复性，那么启用基准测试功能（即设置```torch.backends.cudnn.benchmark = True```）可能会提高性能。
