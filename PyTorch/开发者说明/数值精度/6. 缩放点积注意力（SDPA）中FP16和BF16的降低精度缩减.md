一个简单的SDPA数学后端在使用FP16/BF16输入时，由于使用低精度中间缓冲区，可能会积累显著的数值误差。

为缓解此问题，默认行为现在包括将FP16/BF16输入上转换为FP32。

计算在FP32/TF32中进行，最终的FP32结果随后下转换回FP16/BF16。

这将提高数学后端在FP16/BF16输入情况下最终输出的数值精度，但会增加内存使用量，并且由于计算从FP16/BF16的BMM转向FP32/TF32的BMM/矩阵乘法，可能会导致数学后端的性能下降。

对于那些为了速度而更倾向于使用低精度归约的场景，可以通过以下设置启用它们：

```torch.backends.cuda.allow_fp16_bf16_reduction_math_sdp(True)``
