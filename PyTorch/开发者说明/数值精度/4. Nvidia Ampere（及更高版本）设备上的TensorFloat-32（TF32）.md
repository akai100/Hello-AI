
在Ampere（及后续版本）的英伟达GPU上，PyTorch可以使用TensorFloat32（TF32）来加速数学密集型运算，特别是矩阵乘法和卷积运算。

当使用TF32张量核心执行运算时，仅读取输入尾数的前10位。这可能会降低精度并产生意想不到的结果（例如，将一个矩阵与单位矩阵相乘，得到的结果可能与输入矩阵不同）。

默认情况下，矩阵乘法禁用TF32张量核心，卷积运算启用TF32张量核心，不过大多数神经网络工作负载在使用TF32时与使用fp32时具有相同的收敛行为。

如果你的网络不需要完整的float32精度，建议通过```torch.backends.cuda.matmul.fp32_precision = "tf32"```（```torch.backends.cuda.matmul.allow_tf32 = True```即将被弃用）启用矩阵乘法的TF32张量核心。

如果你的网络在矩阵乘法和卷积运算中都需要完整的float32精度，那么也可以通过```torch.backends.cudnn.conv.fp32_precision = "ieee`"（torch.backends.cudnn.allow_tf32 = False即将被弃用）禁用卷积运算的TF32张量核心
