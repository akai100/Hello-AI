PyTorch 中的许多操作都支持批处理计算，即对输入批次中的元素执行相同的操作。

```torch.mm()``` 和 ```torch.bmm()``` 就是这样的例子。我们可以通过对批次元素进行循环来实现批处理计算，并对各个批次元素应用必要的数学运算，
但出于效率考虑，我们不会这样做，而是通常对整个批次执行计算。

在这种情况下，我们所调用的数学库以及 PyTorch 操作的内部实现，可能会产生与非批处理计算略有不同的结果。

具体来说，设 $A$ 和 $B$ 是维度适合进行批处理矩阵乘法的 3D 张量。那么 $(A@B)[0]$（批处理结果的第一个元素）并不保证与 $A[0]@B[0]$（输入批次第一个元素的矩阵乘积）在按位上完全相同，尽管从数学角度来说，这是相同的计算。
```
由于批量计算和单独计算的内部实现（底层库调用、运算优化策略等）存在差异，再加上浮点数精度有限、运算不满足结合律的特性，
导致两者无法保证位级完全一致（仅存在微小数值差异，并非数学结果错误）。
```

同样地，对张量切片执行的操作，其结果不一定与对完整张量执行相同操作后再取切片的结果完全一致。例如，设A为一个二维张量。```A.sum(-1)[0]``` 不一定与 ```A[:,0].sum()``` 在按位上相等。
