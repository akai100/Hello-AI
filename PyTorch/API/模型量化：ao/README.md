ao（Advanced Optimization，高级优化），是PyTorch 框架中专门用于**深度学习模型高级优化与部署前优化**的核心子库，
其核心目标是在不显著损失模型精度的前提下，提升模型的推理效率、降低模型的存储成本和硬件部署门槛，为模型从训练环境落地
到生产环境提供关键的优化能力支撑。


## 核心子模块

### torch.ao.quantization（模型量化）

+ 核心功能

  将模型的 32 位浮点数（FP32）权重 / 激活值转换为 INT8、INT4 等低精度格式，实现模型压缩与推理加速；

+ 核心工作流

  后训练量化（PTQ，无需重新训练，低成本高效）、量化感知训练（QAT，模拟量化误差训练，高精度）；

+ 核心价值

  8 位量化可将模型体积压缩 1/4，推理速度在 CPU / 边缘设备上提升 2-4 倍，适配端侧 / 边缘端低成本部署

### torch.ao.pruning（模型剪枝）

+ 核心功能

  通过移除模型中 “冗余” 的权重、神经元或通道，实现模型轻量化；

+ 核心类型

  权重剪枝：移除单个不重要的权重参数（如绝对值接近 0 的权重），降低权重矩阵的稀疏度；

+ 结构剪枝

  在保持模型精度的前提下，进一步减小模型体积，降低推理时的内存读取和计算开销，常与量化配合使用，实现 “双重轻量化”

### torch.ao.sparsity

模型稀疏化优化，与剪枝功能互补，专注于通过训练过程诱导模型产生稀疏性（如 L1 正则化约束权重稀疏），生成稀疏模型，适配硬件的稀疏计算加速

### torch.ao,.optim

高级优化器与优化策略，提供比 torch.optim 更精细的训练优化方案，如针对低精度训练的优化器、模型压缩感知的训练策略等，辅助提升量化 / 剪枝后模型的精度；

### torch.ao.onnx

ONNX 格式导出优化，针对量化、剪枝后的优化模型，提供专用的 ONNX 导出接口，解决普通导出可能出现的兼容性问题，方便模型部署到 ONNX Runtime、TensorRT 等推理引擎;

### torch.ao.migration

版本迁移工具，解决不同 PyTorch 版本中 torch.ao 模块 API 变更带来的兼容性问题，保障旧版本优化模型的正常使用与升级
  
