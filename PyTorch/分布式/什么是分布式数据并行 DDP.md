
PyTorch 的 ```DistributedDataParallel```（DDP），它支持 PyTorch 中的数据并行训练。数据并行是一种在多个设备上同时处理多
个数据批次以获得更好性能的方法。在 PyTorch 中，```DistributedSampler``` 确保每个设备获得不重叠的输入批次。模型在所有设
备上进行复制；每个副本计算梯度，并使用[环形全归约算法](https://tech.preferred.jp/en/blog/technologies-behind-distributed-deep-learning-allreduce/)与其他副本同时进行同步。

## 为什么应该优先选择 DDP 而不是 DataParallel(DP)?

```DataParallel`` 是一种较旧的数据并行方法。DP极其简单（只需额外一行代码），但性能要差得多。DDP在几个方面对该架构进行了改进：


## 分布式数据并行（DDP）详细介绍

DDP 的工作流程

1. 初始化与广播

   训练开始时，先在一个节点上初始化模型权重，再把权重同步到所有其他节点。

2. 分数据训练

   每个节点用**相同的初始化权重**，用**数据集的一个子集**上训练模型。

3. 梯度聚合（All-Reduce）

   每训练若干批次后，把所有节点的梯度汇总到一个节点（求和），再将聚合后的梯度同步回所有节点。

4. 更新模型参数

   每个节点用自己的优化器，基于收到的聚合梯度，更新本地模型的参数。

5. 循环训练

   回到步骤 2，重复上述流程，直到训练完成。

## 通信原语

1. 点对点通信

   通信模式类似 “客户端 - 服务器”：一个节点（客户端）只和另一个节点（服务器）进行 “一对一” 的请求 - 响应交互，是单个节点对单个节点的独立通信。

2. 集合通信

   是组级别的通信模式（一个节点对多个节点、或多个节点对一个节点），典型场景是深度学习数据并行训练：

   + 比如一个节点要把初始模型权重 “一次性发给所有其他节点”；
  
   + 或者所有节点把各自的梯度 “汇总到同一个节点”，再接收汇总后的梯度。这类多节点协同的通信，就靠集合通信来实现。
  

## 规约操作

当每个节点处理完若干批数据后，所有节点会将各自计算出的梯度发送到同一个节点，该节点会把这些梯度累加（求和），这个梯度汇总的操作就称为 Reduce。


## 集合通信： All-Reduce

1. 当把所有节点的梯度都汇聚到单个节点后，我们需要将累加后的梯度发送给所有节点。这个操作可以通过广播（Broadcast）算子来完成；

2. “归约 - 广播” 这一操作序列，由另一个名为 All-Reduce 的算子实现 —— 它的运行时间通常比 “先执行归约（Reduce）、再执行广播（Broadcast）” 的分步序列更短；

3. 我不会展示 All-Reduce 背后的具体算法，但你可以把它理解为 “先执行归约、再执行广播” 的操作组合。

## 故障转移：如果一个节点崩溃了会发生什么？

一种办法是重启整个集群，这很简单。但重启集群的话，训练会从零开始，我们会丢失到目前为止所有的参数和计算成果。更好的方案是使用 “检查点（Checkpointing）”。

检查点是指：每隔若干轮迭代（比如每个 epoch），就把模型的权重保存到共享磁盘上；一旦发生崩溃，就能从最近的检查点恢复训练。


### 故障转移：使用检查点

“我们需要共享存储，因为 PyTorch 会决定由哪个节点初始化权重，而我们不应假设具体是哪个节点。因此，每个节点都应能访问共享存储。此外，分布式系统中有一个好原则：不要让某个节点比其他节点更重要 —— 因为任何节点都可能在任何时候发生故障。”


### 故障转移：谁应该保存检查点？

当我们启动集群时，PyTorch 会给每个 GPU 分配一个唯一的 ID（称为 RANK）。我们会这样编写代码：被分配到RANK 0的节点负责保存检查点，这样其他节点就不会互相覆盖对方的文件。因此，只有一个节点负责写入检查点以及训练所需的其他所有文件。

+ RANK 的作用：PyTorch 启动分布式集群时，会给每个 GPU（节点）分配唯一的标识 ID，即 “RANK”；

+ 单一节点负责保存：通过代码指定RANK 0 对应的节点来执行检查点的保存操作；

+ 核心目的：避免多个节点同时写入检查点文件，防止文件被互相覆盖，保证检查点文件的唯一性和完整性。
  
