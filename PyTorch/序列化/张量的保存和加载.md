## 1. 张量的保存和加载

```torch.save()```和```torch.load()```让你可以轻松地保存和加载张量：

```python3
>>> t = torch.tensor([1., 2.])
>>> torch.save(t, 'tensor.pt')
>>> torch.load('tensor.pt')
tensor([1., 2.])
```

按照惯例，PyTorch文件通常以“.pt”或“.pth”为扩展名。

```torch.save()``` 和 ```torch.load()``` 默认使用 Python 的 pickle，因此你也可以将多个张量作为元组、列表和字典等 Python 对象的一部分进行保存：

```python3
>>> d = {'a': torch.tensor([1., 2.]), 'b': torch.tensor([3., 4.])}
>>> torch.save(d, 'tensor_dict.pt')
>>> torch.load('tensor_dict.pt')
{'a': tensor([1., 2.]), 'b': tensor([3., 4.])}
```

如果包含PyTorch张量的自定义数据结构是可 pickle 化的，那么它们也可以被保存。

## 2. 保存和加载张量会保留视图

保存张量会保留它们的视图关系：

```python3
>>> numbers = torch.arange(1, 10)
>>> evens = numbers[1::2]
>>> torch.save([numbers, evens], 'tensors.pt')
>>> loaded_numbers, loaded_evens = torch.load('tensors.pt')
>>> loaded_evens *= 2
>>> loaded_numbers
tensor([ 1,  4,  3,  8,  5, 12,  7, 16,  9])
```

在幕后，这些张量共享相同的“存储”。

PyTorch保存张量时，会将其存储对象和张量元数据分开保存。这是一个实现细节，未来可能会发生变化，但它通常能节省空间，并让PyTorch轻松重建加载的张量之间的视图关系。
例如，在上面的代码片段中，只有一个存储被写入“tensors.pt”。

然而，在某些情况下，保存当前的存储对象可能是不必要的，并且会创建大到难以处理的文件。在下面的代码片段中，一个比所保存的张量大得多的存储被写入到了文件中：

```python3
>>> large = torch.arange(1, 1000)
>>> small = large[0:5]
>>> torch.save(small, 'small.pt')
>>> loaded_small = torch.load('small.pt')
>>> loaded_small.storage().size()
999
```

没有只将小张量中的五个值保存到“small.pt”，而是将它与大张量共享的存储中的999个值都进行了保存和加载。

当保存的张量所含元素少于其存储对象时，可先克隆张量以减小保存文件的大小。克隆张量会生成一个新的张量，该新张量带有一个新的存储对象，且仅包含张量中的值：

```python3
>>> large = torch.arange(1, 1000)
>>> small = large[0:5]
>>> torch.save(small.clone(), 'small.pt')  # saves a clone of small
>>> loaded_small = torch.load('small.pt')
>>> loaded_small.storage().size()
5
```

然而，由于克隆出的张量彼此独立，它们不具备原始张量所具有的任何视图关系。如果在保存小于其存储对象的张量时，文件大小和视图关系都很重要，那么在保存之前，
必须小心构建新的张量，以最小化其存储对象的大小，同时仍保持所需的视图关系。
