# 1. 优化模型参数


# 2. 先决条件代码

# 3. 超参数

超参数是可调整的参数，能够控制模型的优化过程。不同的超参数值会影响模型训练和收敛速度。

我们为训练定义了以下超参数：

+ 时期数 - 遍历数据集的次数

+ 批量大小 - 在更新参数之前，通过网络传播的数据样本数量

+ 学习率 - 每批/每个时期更新模型参数的幅度。较小的值会导致学习速度缓慢，而较大的值可能会在训练过程中导致不可预测的行为


# 4. 优化循环

一旦我们设置好超参数，就可以通过优化循环来训练和优化模型。优化循环的每一次迭代都被称为一个轮次。

每个轮次包含两个主要部分：

+ 训练循环——遍历训练数据集并尝试收敛到最优参数

+ 验证/测试循环——遍历测试数据集以检查模型性能是否在提升


# 5. 损失函数

当提供一些训练数据时，我们未经过训练的网络可能无法给出正确答案。损失函数用于衡量所得结果与目标值的差异程度，而我们在训练过程中就是要最小化这个损失函数。
为了计算损失，我们使用给定数据样本的输入进行预测，并将其与真实数据标签值进行比较。

常见的损失函数包括用于回归任务的```nn.MSELoss```（均方误差），以及用于分类任务的```nn.NLLLoss```（负对数似然）。```nn.CrossEntropyLoss```结合了```nn.LogSoftmax```和```nn.NLLLoss```。

我们将模型的输出logits传递给nn.CrossEntropyLoss，它会对logits进行归一化并计算预测误差。

# 6. 优化器

优化是在每个训练步骤中调整模型参数以减少模型误差的过程。优化算法定义了该过程的执行方式（在本示例中，我们使用随机梯度下降）。所有优化逻辑都封装在optimizer对象中。这里，我们使用SGD优化器；

``python3
我们通过注册模型中需要训练的参数并传入学习率超参数来初始化优化器。
```

在训练循环内部，优化过程分为三个步骤：

+ 调用```optimizer.zero_grad()```来重置模型参数的梯度。梯度默认是累加的；为了避免重复计算，我们在每次迭代时会显式地将其清零；

+ 通过调用```loss.backward()```反向传播预测损失。PyTorch会存储损失相对于每个参数的梯度；

+ 一旦我们获得了梯度，就会调用```optimizer.step()```，通过反向传播中收集的梯度来调整参数;
