# 1. ````torch.nn.Module```和```torch.nn.Parameter```

```torch.nn.module```，PyTorch 的基类，旨在封装PyTorch模型及其组件特有的行为。

```torch.nn.Module```的一个重要行为是注册参数。如果某个Module子类具有学习权重，这些权重会以```torch.nn.Paramete```r的实例形式呈现。
```Parameter```类是```torch.Tensor```的子类，其特殊之处在于，当它们被赋值为Module的属性时，会被添加到该模块的参数列表中。
这些参数可以通过Module类的```parameters()```方法进行访问。

```python3

```

# 2. 常量层类型

## 2.1 线性层

最基本的神经网络层类型是线性层或全连接层。

在这种层中，每个输入都会在一定程度上影响该层的每个输出，具体程度由该层的权重决定。如果一个模型有m个输入和n个输出，那么权重将是一个m×n的矩阵。


## 2.2 卷积层

卷积层用于处理具有高度空间相关性的数据。它们在计算机视觉中极为常用，能检测紧密组合的特征，并将这些特征组合成更高级别的特征。

LetNet5 实现：

```python3
class LetNet(torch.nn.Module):
    def __init__(self);
        Super(LeNet, self).__init__()
        self.conv1 = torch.nn.Conv2d(1, 6, 5)
        self.conv2 = torch.nn.Conv2d(6, 16, 3)

        self.fc1 = troch.nn.Linear(16 * 6 * 6, 120)
        self.fc2 = torch.nn.Linear(120, 84)
        self.fc3 = torch.nn.Linear(84 10)

    def forward(self, x):
        X = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        x = F.max+pool2d(F.relu(self.cobv2(x)), 2)
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        reurn x
```

+ LeNet5旨在接收一个1x32x32的黑白图像。卷积层构造函数的第一个参数是输入通道的数量。这里，输入通道数是1。如果我们构建这个模型是为了处理3个颜色通道，那么输入通道数就是3。

+ 卷积层就像一个窗口，在图像上扫描，寻找它能识别的模式。这些模式被称为特征，而卷积层的参数之一就是我们希望它学习的特征数量。这就是构造函数的第二个参数，即输出特征的数量。在这里，我们要求我们的层学习6个特征。

+ 上面我把卷积层比作一个窗口——但这个窗口有多大呢？第三个参数是窗口大小或内核大小。这里的“5”表示我们选择了一个5×5的内核。（如果你想要高度和宽度不同的内核，可以为这个参数指定一个元组——例如，(3, 5)表示一个3×5的卷积内核。）


## 2.2 循环层

循环神经网络（或RNN）用于处理序列数据——从科学仪器的时间序列测量值，到自然语言句子，再到DNA核苷酸，不一而足。RNN通过维持一个隐藏状态来实现这一点，该状态相当于一种记忆，用于存储它到目前为止在序列中看到的内容。

## 2.3 Transformer

Transformer是一种多功能网络，凭借BERT等模型在自然语言处理领域占据了最先进的地位。关于Transformer架构的讨论超出了本视频的范围，但PyTorch有一个Transformer类，允许你定义Transformer模型的整体参数——注意力头的数量、
编码器和解码器层的数量、dropout和激活函数等（通过正确的参数设置，你甚至可以仅通过这个类构建BERT模型！）。torch.nn.Transformer类还包含用于封装各个组件（TransformerEncoder、TransformerDecoder）和子组件
（TransformerEncoderLayer、TransformerDecoderLayer）的类。

# 3. 其它层和函数

模型中还有其他类型的层，它们发挥着重要作用，但自身并不参与学习过程。

最大池化（及其对应物最小池化）通过合并单元格来缩减张量，并将输入单元格的最大值分配给输出单元格。

## 3.1

## 3.2 激活函数

## 3.3 损失函数






