
神经网络由对数据执行操作的层/模块组成。```torch.nn``` 命名空间提供了构建自己的神经网络所需的所有构建块。PyTorch 中的每个模块都继承自 ```nn.Module```。
神经网络本身就是一个模块，它由其他模块（层）组成。这种嵌套结构便于构建和管理复杂的架构。

# 1. 获取训练设备

我们希望能够在CUDA、MPS、MTIA或XPU等加速器上训练我们的模型。如果当前的加速器可用，我们就会使用它。否则，我们就使用CPU。

```python3
device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else "cpu"
print(f"Using {device} device")
```

# 2. 定义类

我们通过子类化nn.Module来定义神经网络，并在```__init__``中初始化神经网络层。每个```nn.Module``子类都会在```forward```方法中实现对输入数据的操作。

```python3
class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10),
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits
```

我们创建了一个```NeuralNetwork```实例，并将其移植```device```,然后打印结构。

```python3
model = NeuralNetwork().to(device)
print(model)
```

要使用该模型，我们需要向其传入输入数据。这会执行模型的```forward```以及一些后台操作。请勿直接调用```model.forward()```！

在输入上调用该模型会返回一个二维张量，其中dim=0对应每个输出，每个输出包含10个针对每个类别的原始预测值，而dim=1对应每个输出的各个值。我们通过nn.Softmax模块的实例对其进行处理，得到预测概率.

```python3
X = torch.rand(1, 28, 28, device=device)
logits = model(X)
pred_probab = nn.Softmax(dim=1)(logits)
y_pred = pred_probab.argmax(1)
print(f"Predicted class: {y_pred}")
```

# 3. 模型层

## 3.1  nn. Flatten

我们初始化nn.Flatten层，将每个2D的28x28图像转换为一个包含784个像素值的连续数组（保持小批量维度（在dim=0处））。

## 3.2 nn.Linear

线性层是一个使用其存储的权重和偏置对输入进行线性变换的模块


## 3.3 nn. ReLU

非线性激活函数构建了模型输入和输出之间的复杂映射。它们在线性变换之后应用，以引入非线性，帮助神经网络学习各种各样的现象。

在这个模型中，我们在线性层之间使用了nn.ReLU，但还有其他激活函数可以为模型引入非线性。

## 3.4 nn.Sequential

```nn.Sequential``` 是一个有序的模块容器。数据会按照模块定义的顺序通过所有模块。你可以使用序列容器来快速组合出一个网络。

## 3.5 nn.Softmax

神经网络的最后一个线性层返回logits——即[-∞, ∞]范围内的原始值，这些值会被传递到nn.Softmax模块。logits会被缩放至[0, 1]范围内的值，这些值代表模型对每个类别的预测概率。

# 4. 模型参数

神经网络内部的许多层都是参数化的，也就是说，它们具有在训练过程中会被优化的相关权重和偏置。继承```nn.Module``会自动跟踪模型对象内部定义的所有字段，
并通过模型的```parameters()```或```named_parameters()```方法使所有参数可访问。

