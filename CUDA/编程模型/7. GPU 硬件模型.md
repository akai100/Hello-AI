<img width="807" height="1173" alt="image" src="https://github.com/user-attachments/assets/fa3cab5b-6779-446b-996f-9ec086ce8f69" />
# 1. 线程快和网格

当应用程序启动内核时，会启动许多线程，通常是数百万个线程。这些线程被组织成块。一组线程被称为线程块，这或许并不令人意外。线程块被组织成一个网格。
一个网格中的所有线程块都具有相同的大小和维度。图3展示了线程块网格的示意图。

![dd](https://docs.nvidia.com/cuda/cuda-programming-guide/_images/grid-of-thread-blocks.png)

线程块和网格可以是一维、二维或三维的。这些维度有助于简化单个线程到工作单元或数据项的映射。

启动内核时，会使用特定的执行配置来启动，该配置指定了网格和线程块的维度。执行配置还可能包括可选参数，如集群大小、流和SM配置设置，这些将在后面的章节中介绍。

使用内置变量，执行内核的每个线程都可以确定它在其包含块中的位置和它的块在包含网格中的位置。线程也可以使用这些内置变量来确定线程块的维度和启动内核的网格。
这使得每个线程在运行内核的所有线程中都有一个唯一的身份。这个身份经常用于确定线程负责哪些数据或操作。

一个线程块中的所有线程都在单个SM中执行。这使得线程块内的线程能够高效地相互通信和同步。线程块内的所有线程都可以访问片上共享内存，该内存可用于线程块内的线程之间交换信息。

一个网格可能由数百万个线程块组成，而执行该网格的GPU可能只有几十个或几百个流式多处理器（SM）。一个线程块的所有线程都由单个SM执行，并且在大多数情况下[1]，会在该SM上运行至完成。
线程块之间的调度无法得到保证，因此一个线程块不能依赖其他线程块的结果，因为可能要等到该线程块完成后，其他线程块才能被调度。图4展示了一个网格中的线程块如何分配给SM的示例。

![](https://docs.nvidia.com/cuda/cuda-programming-guide/_images/thread-block-scheduling.png)

CUDA编程模型支持任意大的网格在任何规模的GPU上运行，无论GPU只有一个SM还是数千个SM。为实现这一点，除了一些例外情况，CUDA编程模型要求不同线程块中的线程之间不存在数据依赖关系。
也就是说，一个线程不应依赖同一网格中不同线程块内线程的结果，也不应与之同步。一个线程块内的所有线程会同时在同一个SM上运行。网格内的不同线程块会在可用的SM之间进行调度，并且可以按任意顺序执行
。简而言之，CUDA编程模型要求线程块能够以任意顺序执行，无论是并行执行还是串行执行。

## 1.1 线程快集群

除线程块外，计算能力为9.0及更高版本的GPU还具有一个可选的分组级别，称为集群。集群是一组线程块，与线程块和网格一样，它们可以按1、2或3维布局。图5展示了一个线程块网格，该网格也被组织成集群。
指定集群不会改变网格维度或网格内线程块的索引。

![图5 当指定集群时，线程块在网格中处于相同位置，但在其所属的集群中也有一个位置](https://docs.nvidia.com/cuda/cuda-programming-guide/_images/grid-of-clusters.png "dd")

指定集群会将相邻的线程块分组到集群中，并在集群级别提供一些额外的同步和通信机会。具体来说，一个集群中的所有线程块都在单个GPC中执行。图6展示了在指定集群时，线程块如何被调度到GPC中的SM上。
由于线程块是在单个GPC内同时调度的，同一集群中不同块的线程可以使用协作组提供的软件接口进行相互通信和同步。集群中的线程可以访问该集群中所有块的共享内存，这被称为分布式共享内存。
集群的最大大小取决于硬件，并且在不同设备之间有所不同。

图6展示了集群内的线程块如何在GPC内的SM上同时进行调度。集群内的线程块在网格中始终彼此相邻。

![](https://docs.nvidia.com/cuda/cuda-programming-guide/_images/thread-block-scheduling-with-clusters.png)

图6 当指定集群时，集群中的线程块会按照其集群形状在网格内排列。一个集群的线程块会同时调度到单个GPC的SM上

## 1.2 线程束和单指令多线程

在一个线程块中，线程被组织成每组32个线程的结构，称为线程束。线程束以单指令多线程（SIMT）范式执行内核代码。在SIMT中，线程束中的所有线程都在执行相同的内核代码，但每个线程可能通过代码遵循不同的分支。
也就是说，尽管程序的所有线程都执行相同的代码，但线程不需要遵循相同的执行路径。

当线程由线程束执行时，它们会被分配一个线程束通道。线程束通道编号为0到31，线程块中的线程以硬件多线程中详述的可预测方式分配给线程束。

warp中的所有线程会同时执行相同的指令。如果warp中的某些线程在执行时进入控制流分支，而其他线程没有，那么在执行进入分支的线程时，未进入分支的线程会被屏蔽。例如，如果某个条件仅对warp中一半的线程为真，
那么在活跃线程执行这些指令时，warp中的另一半线程会被屏蔽。图7说明了这种情况。当warp中的不同线程执行不同的代码路径时，这种情况有时被称为warp分化。因此，当warp中的线程遵循相同的控制流路径时，GPU的利用率会达到最大化。

![](https://docs.nvidia.com/cuda/cuda-programming-guide/_images/active-warp-lanes.png)

图7 在这个示例中，只有线程索引为偶数的线程会执行if语句体，其他线程在语句体执行期间会被屏蔽

在SIMT模型中，一个线程束中的所有线程都以锁步方式执行内核。硬件执行可能有所不同。有关这种区别的重要性，请参见独立线程执行部分的更多信息。不建议利用线程束执行如何实际映射到真实硬件的相关知识。
CUDA编程模型和SIMT规定，一个线程束中的所有线程共同执行代码。只要遵循编程模型，硬件可以以对程序透明的方式优化被屏蔽的线程通道。如果程序违反此模型，可能会导致未定义行为，且在不同的GPU硬件上可能会有所不同。

*这段内容核心是明确 CUDA 的 SIMT 编程模型规范与硬件实际执行的边界，可从 3 个关键维度简要解析：*

*模型规范：线程束的 “锁步执行” 约定*

*SIMT 模型定义中，一个包含 32 个线程的 “线程束”（warp）必须以 “锁步” 方式执行内核代码 —— 即所有线程同步推进指令（比如同时执行加法、同时判断条件），
即使部分线程因分支（如 if-else）被 “屏蔽”（不参与当前指令），模型层面也要求其跟随整体进度，保证线程束内执行逻辑的一致性。*

*硬件灵活性：透明优化不影响模型合规性*

*实际 GPU 硬件可能不严格按 “纯锁步” 执行（比如对被屏蔽线程的硬件资源做动态调度），但这种优化是 “对程序透明” 的 —— 只要开发者遵循 SIMT 模型规范（如不依赖硬件底层的执行细节），
硬件优化不会改变程序的正确行为。例如，硬件可能让被屏蔽线程暂时释放计算单元，但程序感知不到这种差异，最终结果仍符合模型约定。*

*开发者约束：禁止依赖硬件细节，避免未定义行为*

*规范明确 “不建议利用线程束到硬件的实际映射知识”：若开发者强行假设硬件执行逻辑（比如认为被屏蔽线程会暂停而非跟随锁步），编写依赖硬件细节的代码，会导致 “未定义行为”—— 程序可能在某款 GPU 上正常运行，
但在其他型号 GPU 上崩溃或输出错误结果（因不同硬件的优化逻辑可能不同）。*

虽然在编写CUDA代码时不必考虑线程束，但理解线程束执行模型有助于理解诸如**全局内存合并访问**和**共享内存存储体访问模式**等概念。一些高级编程技术会利用线程块内线程束的特性来减少线程分歧并最大限度地提高利用率。
这种优化及其他优化都利用了线程在执行时会被分组为线程束这一特性。

warp执行的一个含义是，线程块最好指定为线程总数是32的倍数。使用任意数量的线程都是合法的，但当总数不是32的倍数时，线程块的最后一个warp将有一些通道在整个执行过程中未被使用。这可能会导致该warp的功能单元利用率和内存访问不够理想。
