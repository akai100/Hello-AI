掩码语言模型是一种自监督的预训练目标，通过随机 “掩码”（Masking）输入序列中的部分 Token，并让模型预测这些被掩码的 Token，从而学习到丰富的上下文依赖关系和语言表示。

## 1. 核心思想与原理

MLM 的核心思想源于完形填空（Cloze Test）任务。它通过以下步骤工作：

**1. 输出处理**：在模型的输入序列中，随机选择一定比例（例如 15%）的 Token。

**2. Token 替换**：对选中的 Token 进行如下操作：

+ 以 80% 的概率，将其替换为一个特殊的 “[MASK]” 符号

+ 以 10% 的概率，将其替换为一个随机的其他 Token

+ 以 10% 的概率，保持该 Token 不变

**3. 模型预测**： 模型的目标是根据上下文，准确预测出那些被 “[MASK]” 或替换的原始 Token 是什么

**4. 损失计算**：仅计算被掩码或替换位置上的预测损失。

## 2. 关键技术细节

**1. 掩码策略**

+ 随机掩码

  最基础的策略，如 BERT 所用

+ 预测所有 Token

  如 ELECTRA 中的 Generator 部分，预测所有 Token，但训练方式不同

+ SpanBERT

  掩码连续的 Token Span（片段），以更好地学习短语或实体级别的信息

+ ERNIE（百度）

  在 BERT 基础上，增加了对命名实体、短语等的掩码，以学习更高层次的语义信息

**2. 优缺点**

+ 优点

  能学习双向上下文信息，对理解类任务（如文本分类、NER）非常有效

+ 缺点

  预训练和微调之间存在 “预训练 - 微调差异”（Pre-training-Fine-tuning Discrepancy），因为微调时不会遇到 “[MASK]” 符号。
  
