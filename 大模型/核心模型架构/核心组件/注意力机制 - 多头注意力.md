
多头注意力是 Transformer 架构的核心组件，本质是将输入的查询 （Q）、键（K）、值（V）投影到多个**独立的
子空间**（“头”），并行计算多组缩放点积注意力，最后拼接所有头的输出并投影，从而捕捉输入序列中**不同维度
、不同粒度的依赖关系**（比如文本中的语法依赖、语义依赖，或图像中局部/全局特征关联）。

**1. 核心公式**

 $MultiHead(Q, K, V) = Concat(head_1, head_2,...,head_h)W^O$

 $head_i=Attention(QW_{i}^{Q}, KW_{i}^{J}, VW_{i}^{V})=Softmax(\frac{QW_{i}^{Q}\dot (KW_{i}^{K})^T}{\sqrt{d_k}})\dot VW_{i}^{V}$

 + $h$: 头的数量（如 8/12/16, 需满足 $d_{model} % h = 0$）

 + $d_k = d_{model}/h$: 每个头的特征维度

 + $W_{i}^{Q},W_{i}^{K},W_{i}^{V}$: 第 $i$ 个头的投影矩阵

 + $W^O$: 拼接后的输出投影矩阵
