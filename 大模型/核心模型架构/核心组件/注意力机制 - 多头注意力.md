
多头注意力是 Transformer 架构的核心组件，本质是将输入的查询 （Q）、键（K）、值（V）投影到多个**独立的
子空间**（“头”），并行计算多组缩放点积注意力，最后拼接所有头的输出并投影，从而捕捉输入序列中**不同维度
、不同粒度的依赖关系**（比如文本中的语法依赖、语义依赖，或图像中局部/全局特征关联）。

**1. 核心公式**

 $MultiHead(Q, K, V) = Concat(head_1, head_2,...,head_h)W^O$

 $head_i=Attention(QW_{i}^{Q}, KW_{i}^{J}, VW_{i}^{V})=Softmax(\frac{QW_{i}^{Q}\dot (KW_{i}^{K})^T}{\sqrt{d_k}})\dot VW_{i}^{V}$

 + $h$: 头的数量（如 8/12/16, 需满足 $d_{model} % h = 0$）

 + $d_k = d_{model}/h$: 每个头的特征维度

 + $W_{i}^{Q},W_{i}^{K},W_{i}^{V}$: 第 $i$ 个头的投影矩阵

 + $W^O$: 拼接后的输出投影矩阵


**2. 单头注意力的“表达局限”**

单头注意力的核心是将 $Q/K/V$ 投影都**同一个低维空间**计算注意力权重。存在两个关键问题：

+ 维度瓶颈

  若直接用高维（如 512 维）计算，点积结果易过大导致 Softmax 梯度消失；若投影到低维（如 64 维），则只能捕捉单一维度的依赖（比如仅能关注 “相邻词的语法关联”，无法同时关注 “长距离的语义关联”）。

+ 粒度单一

  单头的注意力权重是 “全局平均” 的，无法区分 “哪些位置需要关注局部细节，哪些需要关注全局趋势”。

举例：

+ 句子 “猫抓了老鼠，它很开心”，单头注意力可能仅能捕捉 “它→猫” 的指代关联，却忽略 “抓→老鼠” 的动作 - 对象关联；

+ 而多头注意力可让一个头专注 “指代关联”，另一个头专注 “动作 - 对象关联”，最终融合所有信息。
