
多头注意力是 Transformer 架构的核心组件，本质是将输入的查询 （Q）、键（K）、值（V）投影到多个**独立的
子空间**（“头”），并行计算多组缩放点积注意力，最后拼接所有头的输出并投影，从而捕捉输入序列中**不同维度
、不同粒度的依赖关系**（比如文本中的语法依赖、语义依赖，或图像中局部/全局特征关联）。

**1. 核心公式**

 $MultiHead(Q, K, V) = Concat(head_1, head_2,...,head_h)W^O$

 $head_i=Attention(QW_{i}^{Q}, KW_{i}^{J}, VW_{i}^{V})=Softmax(\frac{QW_{i}^{Q}\dot (KW_{i}^{K})^T}{\sqrt{d_k}})\dot VW_{i}^{V}$

 + $h$: 头的数量（如 8/12/16, 需满足 $d_{model} % h = 0$）

 + $d_k = d_{model}/h$: 每个头的特征维度

 + $W_{i}^{Q},W_{i}^{K},W_{i}^{V}$: 第 $i$ 个头的投影矩阵

 + $W^O$: 拼接后的输出投影矩阵


**2. 单头注意力的“表达局限”**

单头注意力的核心是将 $Q/K/V$ 投影都**同一个低维空间**计算注意力权重。存在两个关键问题：

+ 维度瓶颈

  若直接用高维（如 512 维）计算，点积结果易过大导致 Softmax 梯度消失；若投影到低维（如 64 维），则只能捕捉单一维度的依赖（比如仅能关注 “相邻词的语法关联”，无法同时关注 “长距离的语义关联”）。

+ 粒度单一

  单头的注意力权重是 “全局平均” 的，无法区分 “哪些位置需要关注局部细节，哪些需要关注全局趋势”。

举例：

+ 句子 “猫抓了老鼠，它很开心”，单头注意力可能仅能捕捉 “它→猫” 的指代关联，却忽略 “抓→老鼠” 的动作 - 对象关联；

+ 而多头注意力可让一个头专注 “指代关联”，另一个头专注 “动作 - 对象关联”，最终融合所有信息。


## 优点

### 捕捉不同的表示子空间

多头注意力的一个核心优点是能够并行地在多个不同的表示子空间中进行学习。每个注意力头（head）都可以学习输入数据的不同方面，捕捉到不同的上下文信息或特征。

+ 不同子空间

  每个头有不同的权重矩阵，这使得它们可以从不同的角度关注输入的不同部分。例如，一个头可能专注于捕捉局部依赖关系，而另一个头则可能专注于捕捉长距离依赖关系。

+ 多样性

  多头注意力允许模型从多个角度并行地处理信息，这样可以从多个方面学习数据的特征，增强了模型的表现能力。

### 提高模型的学习能力和灵活性

多头注意力可以显著提升模型的灵活性，使得模型能够同时学习多种模式或依赖关系。

+ 并行计算

  每个头独立计算注意力，因此计算是并行的，这能够提高模型训练和推理的效率。

+ 增强表达能力

  多个注意力头能够捕捉输入数据中不同层次的关系，比如局部关系、全局依赖以及跨句子或跨段落的依赖信息，从而增强了模型对复杂数据的处理能力。

### 避免单一注意力头的瓶颈

如果只使用一个注意力头，模型可能会过于依赖单一的表示，这限制了模型的能力。多头注意力通过将注意力机制分成多个头，避免了这种瓶颈问题。

+ 避免过拟合

  单一头可能会过度拟合某些特征，而多头注意力能够在多种表示空间中并行学习，增加了模型的鲁棒性。

### 更强的上下文建模能力

通过多个头并行处理不同的注意力分布，多头注意力增强了模型在学习长期依赖关系上的能力。

+ 增强上下文感知

多头注意力使得每个头可以专注于不同的上下文部分，从而更好地理解文本的全局和局部信息。例如，一个头可以专注于当前词语的上下文，而另一个头可以关注与其他词语的跨句子依赖

### 提升模型的并行处理能力

由于每个注意力头是独立计算的，Transformer 模型的多头注意力结构能够高效地进行并行处理。

+ 计算效率

  每个头的计算可以在不同的计算单元上并行进行，从而大幅度提升训练和推理的速度。相比传统的循环神经网络（RNN），Transformer 的并行计算能力使得训练过程更加高效。

### 提高模型的表达多样性

多个头能够同时学习不同的注意力模式，使得模型能够表达更多样化的信息。这种多样性对于处理复杂的语言模式或复杂的视觉任务至关重要。

+ 多样性学习

  每个注意力头会学习不同的模式，保证模型能够在多个层次和方向上对输入数据进行建模，能够有效地提升模型在多任务学习中的表现。

### 增强对不同输入的适应性

多头注意力机制通过不同头的并行计算，使得模型能够更灵活地适应不同类型的输入数据。

+ 自适应特征学习

  每个头可能会专注于输入的某个特定特征或模式，这使得多头注意力能够适应不同的输入特征，如短文本、长文本、复杂的语法结构等。

### 帮助捕捉长距离依赖

在传统的模型中，如 RNN，捕捉长距离依赖关系通常需要通过复杂的计算。多头注意力通过并行计算，可以更有效地捕捉长距离的依赖关系。

+ 全局上下文

  在多头注意力机制下，某些注意力头专门捕捉长距离的依赖关系，因此能够更好地捕捉长距离的语义信息。

  
