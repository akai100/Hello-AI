
## 1. 编码器

编码器在 Transformer 模型中用于处理输入数据，将其转换为特征向量表示。在输入经过编码器后，得到了能够抽象表示输入序列信息的
隐向量。这种隐向量可以看作是输入数据的一种压缩、总结与抽象的表达，便于模型后续进行信息处理。

### 1.1 编码器组成

Transformer 中的编码器不止一个，而是由一组 $N$ 个编码器串联组成。一个编码器的输出作为下一个编码器的输入。

编码器模块的主要功能就是提取原句中的特征。


每一个编码器的构造都是相同的，并且包含两个部分：

+ 多头注意力层；

+ 前馈网络层；

## 2. 解码器

基于编码器的输出和已生成的目标序列，自回归式生成目标序列，同时关注输入序列的关键信息和目标序列的上下文连贯性。

### 2.1 解码器结构

**1. 掩码多头自注意力层（Masked Multi-Head Self-Attention）**

与编码器的自注意力层结构类似，但加入了**掩码（Mask）**，作用是防止模型在生成目标序列时看到未来的 Token，保证自回归生成的合理性。

掩码分为两层：

+ 填充掩码

+ 序列掩码

**2. 编码器-解码器主力层**

这是解码器的**跨注意力层**，作用是**让目标序列的每个 Token 关注输入序列的关键信息**（如机器翻译中，目标词关注源语言的对应词）。

+ 该层的 $Q$ 来自掩码自注意力层的输出（目标序列特征），$K,V$ 来自编码器的输出（输入序列特征）

+ 注意力计算方式与多头自注意力层一致，实现了目标序列与输入序列的信息交互；

**3. 位置前馈网络层**

与编码器的 FFN 完全相同，对每个 Token 进行独立的非线性变换。


### 2.2 编码器输出

经过 N 层解码器处理后，输出序列通过一个线性层映射到目标词汇表维度，再通过 softmax 函数得到每个 Token 的概率分布，最终选择概率最大的 Token 作为生成结果。
