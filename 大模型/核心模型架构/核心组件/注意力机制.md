注意力机制是深度学习中一种模拟人类选择性注意力的核心技术，其核心思想是：**让模型在处理输入序列时，动态地分配不同权重给不同的输入部分，重点关注与当前任务更相关的信息**，从而提升模型对关键特征的捕捉能力。

## 1. 核心原理

注意力机制的数学本质是加权求和，步骤可概括为：

**1. 计算注意力分数**

衡量查询向量（Query）与键向量（Key）之间的相似度；

**2. 归一化分数**

通过 Softmax 函数将分数转化为 0 ~ 1 的权重，确保权重之和为 1；

**3. 加权求和值向量**

用归一化后的权重对值向量（Value）加权，得到注意力输出；


### 1.1 基础公式

$Attention(Q, K, V) = Softmax(\frac{QK^T}{\sqrt{d_k}})V$

+ $Q(Query)$

  查询向量，代表当前需要处理的信息；

+ $K(key)$

  键向量，代表输入的候选信息；

+ $V(Value)$

  值向量，代表与键对应的具体内容

+ $d_k$

  Q/K 的维度，缩放因子 {\sqrt{d_k}} 用于解决高维空间下内积过大导致 Softmax 饱和的问题

## 2. 注意力机制的分类

### 2.1 按结构分类

+ 自注意力

  Q/K/V 均来自同一输入序列，捕捉序列内部的依赖关系

  文本理解、机器翻译（Transformer 编码器）

+ 交叉注意力

  Q 来自目标序列，K/V 来自源序列，建立源 - 目标序列的关联

  机器翻译、图文生成（Transformer 解码器）

+ 多头注意力

  将 Q/K/V 拆分为多个子空间，并行计算多组注意力，再拼接输出

  捕捉不同维度的特征关联（Transformer 核心组件）

### 2.2 按计算方式分类

+ 加性注意力（Additive Attention）

  通过前馈网络计算 Q 与 K 的相似度，适用于 Q/K 维度不同的场景，公式为：

  $Score(Q, K)=V^T tanh(W_qQ+W_kK)$

+ 点积注意力

  直接计算 Q 与 K 的内积，计算效率更高，Scaled Dot-Product Attention 是其优化版本。

+ 多头注意力

  
  
