
FlashAttention 是由斯坦福大学提出的**高效注意力计算算法**，核心解决传统多头注意力中“高显存占用、低访存效率”的问。

**1. 传统注意力的显存/效率问题**

+ 关键问题1

  $QK^T$ 会生成形状为 $(N, N)$ 的注意力分数矩阵，当 $N = 1024$ 时的矩阵大小为 $1024 \mult 2024 = 1M$

+ 关键问题 2

  传统计算需多次读写显存（Q/K/V→分数矩阵→Softmax→加权 V），访存开销远大于计算开销，GPU 算力利用率低;

**2. 核心原理：分块 + 重排 + 融合**

FlashAttention 通过**分块（Blocking）** 、**显存层级重排** 和 **计算 - 访存融合** 三大核心优化，将注意力计算的显存复杂度从 $O(N^2)$ 降至 $O(N)$ ，同时提升 GPU 访存效率：


+ 核心思路

  + 把长序列的 Q/K/V 拆分为多个小 “块（Block）”，
 
  + 仅将当前计算所需的块加载到 GPU 高速缓存（Shared Memory）
 
  + 同时将 Softmax、加权求和等步骤融合到块计算中，减少显存读写次数

+ 简化流程

  + 分块加载 Q/K 到 Shared Memory（缓存）
 
  + 块内计算分数 → 仅保留块内最大值 / 总和（用于 Softmax 归一化）
 
  + 块内计算 Softmax（利用缓存的最大值 / 总和） → 无需存储完整权重矩阵
 
  + 块内加权求和 V → 结果累加后写回显存
