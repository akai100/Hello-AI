{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from tensorboard.compat.tensorflow_stub.dtypes import bfloat16"
   ],
   "id": "b0d889bdcab0d9af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": "def main():",
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6e3a9b84d5a110dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def tensors_memory():",
   "id": "4064c5bf97f06f11",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "几乎所有东西（参数、梯度、激活值、优化器状态）都存储为浮点数。\n",
    "\n",
    "# float32\n",
    "float32 数据类型（也称为 fp32 或单精度）是默认类型。\n",
    "传统上，在科学计算中，float32 是基准；在某些情况下，你可以使用双精度（float64）。\n",
    "在深度学习领域，你可以不用那么严谨。\n",
    "让我们检查一下这些张量的内存使用情况。\n",
    "内存由（i）值的数量和（ii）每个值的数据类型决定。\n"
   ],
   "id": "477830aeba6a4799"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    x = torch.zeros(4, 8)\n",
    "    assert x.dtype == torch.float32\n",
    "    assert x.numel() == 4 * 8\n",
    "    assert x.element_size() == 4\n",
    "    assert get_memory_usage(x) == 4 * 8 * 4"
   ],
   "id": "15305ccd40b5e721",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "GPT-3 前馈层中的一个矩阵：",
   "id": "7fa53c3d62a7975a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "assert get_memory_usage(torch.empty(12288 * 4, 12288)) == 2304 * 1024 * 1024 # 2.3 GB",
   "id": "66ecee49159d4c68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# float16\n",
    "sign(1B) + exponent(5B) + fraction(10B)\n",
    "float16 数据类型（也称为 fp16 或半精度）减少了内存占用。\n"
   ],
   "id": "3553e873700eec5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.zeros(4, 8, dtype=torch.float16)\n",
    "assert x.element_size() == 2\n"
   ],
   "id": "53da545e152096b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "然而，动态范围（尤其是对于小数字而言）并不理想。",
   "id": "306590a10e92347f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.tensor([1e-8], dtype=torch.float16)\n",
    "assert x == 0"
   ],
   "id": "a7f4cfc8feed7aec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "如果在训练时发生这种情况，可能会导致不稳定性。",
   "id": "eba670bb360d8a20"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# bfloat16\n",
    "Sign(1B) + exponent(8B) + fraction(7B)\n",
    "谷歌大脑（Google Brain）于 2018 年开发了 bfloat（脑浮点）来解决这一问题。\n",
    "bfloat16 使用与 float16 相同的内存，但具有与 float32 相同的动态范围！\n",
    "唯一的问题是分辨率较低，但这对深度学习来说影响较小。\n"
   ],
   "id": "cb9999e697b26792"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.tensor([1e-8], dtype=torch.bfloat16)\n",
    "assert x != 0"
   ],
   "id": "2c3c776c0f4471c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "让我们比较一下不同数据类型的动态范围和内存使用情况：",
   "id": "77b05c2a37878e09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "float32_info = torch.finfo(torch.float32)\n",
    "print(float32_info)\n",
    "float16_info = torch.finfo(torch.float16)\n",
    "print(float16_info)\n",
    "bfloat16_info = torch.finfo(torch.bfloat16)\n",
    "print(bfloat16_info)"
   ],
   "id": "32a0d4463da21b64",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# fp8\n",
    "FP8 E4M3: sign(1B) + exponent(4B) + fraction(3B)\n",
    "FP8 E5M2: sign(1B) + exponent(5B) + fraction(2B)\n",
    "2022 年，受机器学习工作负载的推动，FP8 实现了标准化。\n",
    "H100 支持两种 FP8 变体：E4M3（范围 [-448，448]）和 E5M2（[-57344，57344]）。\n",
    "\n",
    "对训练的影响：\n",
    "(1)使用 float32 进行训练是可行的，但需要大量内存。\n",
    "(2)使用 fp8、float16 甚至 bfloat16 进行训练是有风险的，可能会出现不稳定性。\n",
    "(3)解决方案（稍后）：使用混合精度训练，参见 [mixed_precision_training]()\n",
    "\n",
    "\n"
   ],
   "id": "317d2a7c6cd95055"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def tensors_on_gpus():\n",
   "id": "d5d90c974e8ec986",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "默认情况下，张量存储在 CPU 内存中。",
   "id": "12f0088f27d5d132"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.zeros(32, 32)\n",
    "assert x.device == torch.device('cpu')"
   ],
   "id": "9bb68d543608caf2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "然而，为了利用 GPU 的大规模并行性，我们需要将它们移至 GPU 内存中。",
   "id": "201ca02f1d7042ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "我们先看看有没有任何 GPU。",
   "id": "f437f2fe43bb5f0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not torch.cuda.is_available():\n",
    "    return\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "for i in range(num_gpus):\n",
    "    properties = torch.cuda.get_device_properties(i)\n",
    "\n",
    "memory_allocated = torch.cuda.memory_allocated()\n",
    "\n",
    "text(\"Move the tensor to GPU memory (device 0).\")\n",
    "y = x.to(\"cuda:0\")\n",
    "assert y.device == torch.device('cuda', 0)\n",
    "\n",
    "text(\"Or create a tensor directly on the GPU.\")\n",
    "z = torch.zeros(32, 32, device=\"cuda:0\")\n",
    "\n",
    "new_memory_allocated = torch.cuda.memory_allocated()\n",
    "memory_used = new_memory_allocated - memory_allocated\n",
    "assert memory_used == 2 * (32 * 32 * 4)"
   ],
   "id": "498bfd5ff16fc9e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def tensor_operations():\n",
   "id": "2158c93b795aeae3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "大多数张量是通过对其他张量执行运算而创建的。\n",
    "每个操作都有一定的内存和计算后果。"
   ],
   "id": "4d34cbc7356eacb0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tensor_storage()\n",
    "tensor_slicing()\n",
    "tensor_elementwise()\n",
    "tensor_matmul()"
   ],
   "id": "613256a066b6e26a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def tensor_storage():\n",
   "id": "1f470d5295fbca53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "PyTorch 中的张量是什么？\n",
    "\n",
    "PyTorch 张量是指向已分配内存的指针\n",
    "\n",
    "…… 以及描述如何获取张量中任意元素的元数据。\n"
   ],
   "id": "a2ed548a2ed48bc9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.tensor([\n",
    "    [0, 1, 2, 3],\n",
    "    [4, 5, 6, 7],\n",
    "    [8, 9, 10, 11],\n",
    "    [12, 13, 14, 15]\n",
    "])"
   ],
   "id": "8d0cdb23ed93e355",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "要进入下一行（维度 0），在存储中跳过 4 个元素。",
   "id": "38104fbf055ee86a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "assert x.stride(0) == 4",
   "id": "ef945920f7c58d60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "要转到下一列（维度 1），需在存储中跳过 1 个元素。",
   "id": "a46998cf7ec21555"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "assert  x.stride(1) == 1",
   "id": "e16ffc71887b53b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "查找元素",
   "id": "a09541d34ab5a88a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " r, c = 1, 2\n",
    "index = r * x.stride(0) + c * x.stride(1)  # @inspect index\n",
    "assert index == 6"
   ],
   "id": "22ba10a524001033",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def tensor_slicing():\n",
    "    x = torch.tensor([[1, 2, 3], [4, 5, 6]])"
   ],
   "id": "9978d5c8cf87c134",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "许多操作只是提供了张量的不同视图。\n",
    "\n",
    "这不会创建副本，因此一个张量中的突变会影响另一个张量。"
   ],
   "id": "186b5c71f72e365f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "获取 0 行",
   "id": "29cc4a5893f4d4ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y = x[0]\n",
    "assert torch.equal(y, torch.tensor([1, 2, 3]))\n",
    "assert same_storage(x, y)"
   ],
   "id": "d5fc23076750e700",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "获取第1行",
   "id": "ffea7a00b39d3da3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y = x[:, 1]\n",
    "assert torch.euqal(y, torch.tensor([2, 5]))\n",
    "assert same_storage(x, y)"
   ],
   "id": "5c2ebaa283bded81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "view",
   "id": "15123aabc0846233"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y = x.view(3, 2)\n",
    "assert torch.equal(y, torch.tensor([[1, 2], [3, 4], [5, 6]]))\n",
    "assert same_storage(x, y)"
   ],
   "id": "2b884f87ddac74c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "矩阵转置",
   "id": "a33aafe74dd9d44c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y = x.transpose(1, 0)\n",
    "assert torch.equal(y, torch.tensor([[1, 4], [2, 5], [3, 6]]))\n",
    "assert same_storage(x, y)"
   ],
   "id": "8aaeb2223a7e95f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "校验操作x就是操作y\n",
   "id": "f76c96f153ab29ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x[0][0] = 100\n",
    "assert y[0][0] == 100"
   ],
   "id": "3d84173ca9fbb025",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "请注意，有些视图是不连续的条目，这意味着无法进行更多的视图操作。",
   "id": "43c640cdc3ed2231"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.tensor([[1., 2, 3], [4, 5, 6]])\n",
    "y = x.transpose(1, 0)\n",
    "assert not y.is_contiguous()\n",
    "try:\n",
    "    y.view(2, 3)\n",
    "    assert False\n",
    "except RuntimeError as e:\n",
    "    assert \"view size is not compatible with input tensor's size and stride\" in str(e)"
   ],
   "id": "8dd174acc482bd80",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "可以先强制一个张量为连续的：",
   "id": "aa7852646432131"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y = x.transpose(1, 0).contiguous().view(2, 3)\n",
    "assert not same_storage(x, y)"
   ],
   "id": "933b4eb93a604665",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "视图是免费的，复制则会占用（额外的）内存和计算资源。",
   "id": "e470edb140601877"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def tensor_elementwise():",
   "id": "e8cf93988e6dcb73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "这些操作会对张量的每个元素执行某种运算\n",
    "... 并返回一个（新的）形状相同的张量。\n"
   ],
   "id": "9779a30c4843a22d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.tensor([1, 4, 9])\n",
    "assert torch.equal(x.pow(2), torch.tensor([1, 16, 81]))\n",
    "assert torch.equal(x.sqrt(), torch.tensor([1, 2, 3]))\n",
    "assert torch.equal(x.rsqrt(), torch.tensor([1, 1 / 2, 1 / 3]))\n",
    "\n",
    "assert torch.equal(x + x, torch.tensor([2, 8, 18]))\n",
    "assert torch.equal(x * 2, torch.tensor([2, 8, 18]))\n",
    "assert torch.equal(x / 0.5, torch.tensor([2, 8, 18]))"
   ],
   "id": "31ca00c32a7f8549",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "triu 函数取矩阵的上三角部分",
   "id": "917e84c430e11079"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " x = torch.ones(3, 3).triu()  # @inspect x\n",
    "assert torch.equal(x, torch.tensor([\n",
    "    [1, 1, 1],\n",
    "    [0, 1, 1],\n",
    "    [0, 0, 1]],))"
   ],
   "id": "1cc91d7ccfb8d4aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "这对于计算因果注意力掩码很有用，其中 M [i, j] 是 i 对 j 的贡献。",
   "id": "68948d47b8e0a1c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def tensor_matmul():",
   "id": "e149d7513f99a3b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    最后，深度学习的核心所在：矩阵乘法",
   "id": "5b81b72730d3a34d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    x = torch.ones(16, 32)\n",
    "    w = torch.ones(32, 2)\n",
    "    y = x @ w\n",
    "    assert y.size() == torch.Size([16, 2])"
   ],
   "id": "fe5332018d3ddb0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    通常，我们会对批次中的每个样本和序列中的每个标记执行操作。",
   "id": "957cae98901d1e53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    x = torch.ones(4, 8, 16, 32)\n",
    "    w = torch.ones(32, 2)\n",
    "    y = x @ w\n",
    "    assert y.size() == torch.Size([4, 8, 16, 2])"
   ],
   "id": "494604745437141b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    在这种情况下，我们遍历 x 的前两个维度的值，并将其与 w 相乘。",
   "id": "f9d61736061214c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def tensor_einops():\n",
    "    einops_motivation()"
   ],
   "id": "4d5b98deb205d585",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    Einops 是一个用于处理具有命名维度的张量的库.\n",
    "\n",
    "    它的灵感来源于爱因斯坦求和符号（爱因斯坦，1916 年）。"
   ],
   "id": "e12dec304fa119"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    jaxtyping_basics()\n",
    "    einops_einsum()\n",
    "    einops_reduce()\n",
    "    einops_rearrange()"
   ],
   "id": "93f95148c6d0f76c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def einops_motivation():",
   "id": "6bc2f000328f1c04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    传统的 PyTorch 代码：\n",
   "id": "7a8351d9a6f1d1fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    x = torch.ones(2, 2, 3)  # batch, sequence, hidden  @inspect x\n",
    "    y = torch.ones(2, 2, 3)  # batch, sequence, hidden  @inspect y\n",
    "    z = x @ y.transpose(-2, -1)  # batch, sequence, sequence  @inspect z"
   ],
   "id": "fce4e1347049ac02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    很容易弄混维度（-2、-1 是什么？）……",
   "id": "8538f4d612c17ef1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def jaxtyping_basics():",
   "id": "8b49e249315d6627",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    你是如何跟踪张量维度的？\n",
    "\n",
    "    原始方法\n"
   ],
   "id": "bfea3b58c1c435bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "    x = torch.ones(2, 2, 1, 3)",
   "id": "9a2843f8a4c24d9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    新（jaxtyping）方式",
   "id": "f47f2329ba05eba8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "    x: Float[torch.Tensor, \"batch seq heads hidden\"] = torch.ones(2, 2, 1, 3)  # @inspect x",
   "id": "8e4b9467c1e3731e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    注意：这只是文档说明（无强制力）。",
   "id": "a5d1998c6048f26c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def einops_einsum():",
   "id": "372f839c58b42cc5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    爱因斯坦求和（Einsum）是具有良好记账功能的广义矩阵乘法。\n",
    "\n",
    "    定义两个张量\n"
   ],
   "id": "980f10a190fb7d89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "     x: Float[torch.Tensor, \"batch seq1 hidden\"] = torch.ones(2, 3, 4)\n",
    "     y: Float[torch.Tensor, \"batch seq2 hidden\"] = torch.ones(2, 3, 4)"
   ],
   "id": "ecab81427e7796e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    老方法：",
   "id": "876a5f2b1c275af0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "    z = x @ y.transpose(-2, -1)",
   "id": "18e0188a3778928b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    新(einops) 方法:",
   "id": "b78ef3cd58cde978"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "    z = einsum(x, y, \"batch seq1 hidden, batch seq2 hidden -> batch seq1 seq2\")",
   "id": "81ad0fd183ec9aa8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    未在输出中命名的维度会被求和。\n",
    "\n",
    "\n",
    "    或者可以使用…… 来表示在任意数量的维度上进行广播："
   ],
   "id": "50df17ab1dc88484"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "    z = einsum(x, y, \"... seq1 hidden, ... seq2 hidden -> ... seq1 seq2\")",
   "id": "e7a67ad66e021e56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def einops_reduce():",
   "id": "d11fafe711f18516",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    你可以通过某些运算（例如求和、求均值、求最大值、求最小值）对单个张量进行缩减。",
   "id": "7e938cf9152f9f71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "    x: Float[torch.Tensor, \"batch seq hidden\"] = torch.ones(2, 3, 4)",
   "id": "cb2749bee75db74a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    老方法",
   "id": "7471191bad8135d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "    y = x.mean(dim=-1)",
   "id": "e12dffcd8ddf332d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    新 (einops) 实现方式:",
   "id": "1587737d82e67537"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "    y = reduce(x, \"... hidden -> ...\", \"sum\")",
   "id": "8c6534a75790354c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def einops_rearrange():",
   "id": "223d282a700b0f44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    有时候，一个维度代表着两个维度\n",
    "    …… 并且你想要对其中一个进行操作。"
   ],
   "id": "f9e0d6e6dbe0f483"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "    x: Float[torch.Tensor, \"batch seq total_hidden\"] = torch.ones(2, 3, 8)",
   "id": "1ba91e82e897e714",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    ... 其中 total_hidden 是 heads * hidden1 的扁平化表示",
   "id": "b4e6c2004cbd7578"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "    w: Float[torch.Tensor, \"hidden1 hidden2\"] = torch.ones(4, 4)",
   "id": "135b8026527c3d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    将 total_hidden 拆分为两个维度（头数和 hidden1）：",
   "id": "7c0aa6a2bfc2fefc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "    x = rearrange(x, \"... (heads hidden1) -> ... heads hidden1\", heads=2)",
   "id": "be17da9d8c34d9f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    通过 w 执行转换：",
   "id": "6d5adee7293be32b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "    x = einsum(x, w, \"... hidden1, hidden1 hidden2 -> ... hidden2\")",
   "id": "e7bd5a9e22c671dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    将头和 hidden2 重新组合在一起：",
   "id": "dbb11dd5f7b8bd67"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "    x = rearrange(x, \"... heads hidden2 -> ... (heads hidden2)\")",
   "id": "bce6aad1c1b3fbf2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def tensor_operations_flops():",
   "id": "81e034d9ba41764c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    在完成所有操作后，让我们检查一下它们的计算成本。\n",
    "\n",
    "    浮点运算（FLOP）是一种基本运算，例如加法（x + y）或乘法（x y）。\n",
    "\n",
    "    两个极其令人困惑的首字母缩写词（发音相同！）：\n",
    "\n",
    "    （1）FLOPs：浮点运算（衡量已完成的计算量）\n",
    "\n",
    "    （2）FLOP/s：每秒浮点运算次数（也写作 FLOPS），用于衡量硬件的速度。\n",
    "\n",
    "    # 直觉\n",
    "\n",
    "    训练 GPT-3（2020 年）需要 3.14e23 次浮点运算；\n",
    "\n",
    "    据推测，训练 GPT-4（2023 年）需要 2e25 次浮点运算\n",
    "\n",
    "    美国行政命令：任何使用≥1e26 次浮点运算训练的基础模型都必须向政府报告\n",
    "\n",
    "    A100 的峰值性能为 312 万亿次浮点运算 / 秒"
   ],
   "id": "d476cbf1d4f40e68"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "    assert a100_flop_per_sec == 312e12",
   "id": "f58c41f9a08fb870",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    H100 在启用稀疏性时的峰值性能为 1979 万亿次每秒浮点运算，不启用时为 50%",
   "id": "bb987c08a74ea12a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "    assert h100_flop_per_sec == 1979e12 / 2",
   "id": "2e690cf68138a8ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 线性模型\n",
    "\n",
    "    作为动机，假设你有一个线性模型\n",
    "\n",
    "   + 我们有 n 个点\n",
    "\n",
    "   + 每个点都是 d 维的\n",
    "\n",
    "   + 线性模型将每个 d 维向量映射到 k 个输出\n",
    "\n"
   ],
   "id": "8e110b5f0da8e0cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    if  torch.cuda.is_available():\n",
    "        B = 16384\n",
    "        D = 32768\n",
    "        K = 8192\n",
    "    else:\n",
    "        B = 1024\n",
    "        D = 256\n",
    "        K = 64\n",
    "\n",
    "    device = get_device()\n",
    "    x = torch.ones(B, D, device=device)\n",
    "    w = torch.randn(D, K, device=device)\n",
    "    y = x @ w"
   ],
   "id": "becc338b3ac8ddf1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    我们每个（i，j，k）三元组有一次乘法运算（x [i][j] * w [j][k]）和一次加法运算。",
   "id": "5e488c00591ae08f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "     actual_num_flops = 2 * B * D * K",
   "id": "638c9bb5c08614a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 其他操作的浮点运算次数\n",
    "\n",
    "+ 对一个 m×n 矩阵进行逐元素运算需要 O (mn) 次浮点运算;\n",
    "\n",
    "+ 两个 m×n 矩阵相加需要 m×n 次浮点运算\n",
    "\n",
    "一般来说，在深度学习中，对于足够大的矩阵，你所遇到的其他任何运算都不会像矩阵乘法那样耗费资源。\n",
    "\n",
    "解释：\n",
    "\n",
    "+ B 是数据点的数量；\n",
    "\n",
    "+ （D K）是参数的数量；\n",
    "\n",
    "+ 前向传播的浮点运算次数（FLOPs）为 2×（令牌数量）×（参数数量）\n",
    "\n",
    "事实证明，这可以推广到 Transformer 模型（作为一级近似\n",
    "\n",
    "我们的 FLOPs 计算如何转换为实际运行时间（秒）？\n",
    "\n",
    "我们来计时吧！\n"
   ],
   "id": "d2a0c77e6f2a5292"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " actual_time = time_matmul(x, w)\n",
    " actual_flop_per_sec = actual_num_flops / actual_time"
   ],
   "id": "67a3d19f36ef0c72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "每个 GPU 都有一份规格表，上面标注了峰值性能\n",
    "\n",
    "+ A100\n",
    "\n",
    "+ H100\n",
    "\n",
    "请注意，每秒浮点运算次数（FLOP/s）很大程度上取决于数据类型！"
   ],
   "id": "5269555e0f211363"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "    promised_flop_per_sec = get_promised_flop_per_sec(device, x.dtype)",
   "id": "9a869f8bd91a9f6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 模型浮点运算次数利用率（MFU）\n",
    "\n",
    "定义：（实际每秒浮点运算次数）/（承诺的每秒浮点运算次数）"
   ],
   "id": "7142d46feca0e16d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": " mfu = actual_flop_per_sec / promised_flop_per_sec",
   "id": "b667e191606fa3fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "通常，MFU 大于或等于 0.5 就相当不错了（如果矩阵乘法占主导地位，MFU 会更高）\n",
    "\n",
    "让我们用 bfloat16 来做："
   ],
   "id": "374e64d70a1ca4d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    x = x.to(torch.bfloat16)\n",
    "    w = w.to(torch.bfloat16)\n",
    "    bf16_actual_time = time_matmul(x, w)\n",
    "    bf16_actual_flop_per_sec = actual_num_flops / bf16_actual_time\n",
    "    bf16_promised_flop_per_sec = get_promised_flop_per_sec(device, x.dtype)\n",
    "    bf16_mfu = bf16_actual_flop_per_sec / bf16_promised_flop_per_sec"
   ],
   "id": "37ad2a7339cb59b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "注意：与 float32 相比，bfloat16 的实际每秒浮点运算次数（FLOP/s）更高。\n",
    "\n",
    "这里的 MFU 相当低，可能是因为承诺的 FLOPs 有点过于乐观了。\n",
    "\n",
    "# 总结\n",
    "\n",
    "+ 矩阵乘法占主导地位：（2 m n p）次浮点运算\n",
    "\n",
    "+ 每秒浮点运算次数（FLOP/s）取决于硬件（H100 远优于 A100）和数据类型（bfloat16 远优于 float32）\n",
    "\n",
    "+ 模型浮点运算利用率（MFU）：（实际每秒浮点运算次数）/（标称每秒浮点运算次数）"
   ],
   "id": "d60a9f6540e5a42d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def gradients_basics():",
   "id": "bd7fe2b6e6a30918",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    到目前为止，我们已经构建了张量（它们要么对应参数，要么对应数据），并通过运算（前向传播）传递了这些张量。\n",
    "\n",
    "    现在，我们要计算梯度（反向传播）\n",
    "\n",
    "    作为一个简单的例子，让我们来考虑这个简单的线性模型：\n",
    "\n",
    "    y = 0.5 (x * w - 5)^2\n",
    "\n",
    "    前向传播：计算损失\n"
   ],
   "id": "9c9b04765afdba26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    x = torch.tensor([1., 2, 3])\n",
    "    w = torch.tensor([1., 1, 1], requires_grad=True)  # Want gradient\n",
    "    pred_y = x @ w\n",
    "    loss = 0.5 * (pred_y - 5).pow(2)"
   ],
   "id": "3307d4b42bbc2738",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    反向传播：计算梯度",
   "id": "335978d7bb549a08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    loss.backward()\n",
    "    assert loss.grad is None\n",
    "    assert pred_y.grad is None\n",
    "    assert x.grad is None\n",
    "    assert torch.equal(w.grad, torch.tensor([1, 2, 3]))"
   ],
   "id": "4354bfd8e16edcf5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def gradients_flops():",
   "id": "18e6240312af4627",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    让我们来计算梯度计算的浮点运算次数（FLOPs）\n",
    "\n",
    "\n",
    "    重新审视我们的线性模型"
   ],
   "id": "92e19a0790d2ac1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    if torch.cuda.is_available():\n",
    "        B = 16384\n",
    "        D = 32768\n",
    "        K = 8192\n",
    "    else:\n",
    "        B = 1024\n",
    "        D = 256\n",
    "        K = 64\n",
    "\n",
    "    device = get_device()\n",
    "    x = torch.ones(B, D, device=device)\n",
    "    w1 = torch.randn(D, D, device=device, requires_grad=True)\n",
    "    w2 = torch.randn(D, K, device=device, requires_grad=True)"
   ],
   "id": "789c25093a92b8a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    Model: x --w1--> h1 --w2--> h2 -> loss",
   "id": "f76ab96c07454b65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    h1 = x @ w1\n",
    "    h2 = h1 @ w2\n",
    "    loss = h2.pow(2).mean()"
   ],
   "id": "d5500f6e6409ea87",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    回顾前向 FLOPs 的数量：tensor_operations_flops\n",
    "\n",
    "    + 计算 x [i][j] 乘以 w1 [j][k]\n",
    "\n",
    "    + 加上 h1 [i][k]\n",
    "\n",
    "    + 将 h1 [i][j] 乘以 w2 [j][k]\n",
    "\n",
    "    +  加上h2[i][k]"
   ],
   "id": "2881c379cb69e9a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "    num_forward_flops = (2 * B * D * D) + (2 * B * D * K)",
   "id": "c5b8343ddc988194",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    运行反向传播需要多少 FLOPs？",
   "id": "44d98768276472d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    h1.retain_grad()\n",
    "    h2.retain_grad()\n",
    "    loss.backward()"
   ],
   "id": "ea978fe9081681bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "召回模型：x --w1--> h1 --w2--> h2 -> 损失\n",
    "\n",
    "+ h1.grad = d loss / d h1\n",
    "\n",
    "+ h2.grad = d loss / d h2\n",
    "\n",
    "+ w1.grad = d loss / d w1\n",
    "\n",
    "+ w2.grad = d loss / d w2"
   ],
   "id": "767e29e821deb160"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "关注参数 w2。\n",
    "\n",
    "应用链式法则。"
   ],
   "id": "5d8d291024d4db62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "num_backward_flops = 0",
   "id": "23308e01d81844eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "w2.grad[j,k] = sum_i h1[i,j] * h2.grad[i,k]",
   "id": "47525127d041cec1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    assert w2.grad.size() == torch.Size([D, K])\n",
    "    assert h1.size() == torch.Size([B, D])\n",
    "    assert h2.grad.size() == torch.Size([B, K])"
   ],
   "id": "16bb61264247a926",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For each (i, j, k), multiply and add.",
   "id": "787921c25c1cc015"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "     num_backward_flops += 2 * B * D * K",
   "id": "39906f5126ede347",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "      h1.grad[i,j] = sum_k w2[j,k] * h2.grad[i,k]\n",
   "id": "7f70cf17cf201493"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    assert h1.grad.size() == torch.Size([B, D])\n",
    "    assert w2.size() == torch.Size([D, K])\n",
    "    assert h2.grad.size() == torch.Size([B, K])"
   ],
   "id": "c31c42a84bfb2d88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "对于每一个（i，j，k），进行乘法和加法运算。",
   "id": "6f024d3968059219"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "num_backward_flops += 2 * B * D * K",
   "id": "9e34d3a83ed48ef4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "这只是针对 w2（D*K 参数）的情况\n",
    "\n",
    "对于 w1（D*D 参数）也可以做到（不过不需要 x.grad）"
   ],
   "id": "83588f2ef744513f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "num_backward_flops += (2 + 2) * B * D * D",
   "id": "20bba4225cad5f6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def module_parameters():\n",
    "    input_dim = 16384\n",
    "    output_dim = 32"
   ],
   "id": "7c5a50ca024f4a35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    模型参数在 PyTorch 中存储为 nn.Parameter 对象。",
   "id": "4fc424a05eb09d5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    w = nn.Parameter(torch.randn(input_dim, output_dim))\n",
    "    assert isinstance(w, torch.Tensor)\n",
    "    assert type(w.data) == torch.Tensor"
   ],
   "id": "4c011b384cb5c47d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 参数初始化\n",
    "\n",
    "让我们看看会发生什么。"
   ],
   "id": "341a946fac1a883d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    x = nn.Parameter(torch.randn(input_dim))\n",
    "    output = x @ w\n",
    "    assert output.size() == torch.Size([output_dim])"
   ],
   "id": "cb2e3939b4973212",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "请注意，输出的每个元素都按输入维度的平方根缩放：18.919979095458984。\n",
    "\n",
    "较大的值可能会导致梯度爆炸，进而使训练不稳定。\n",
    "\n",
    "我们希望有一种不受 input_dim 影响的初始化方式。\n",
    "\n",
    "要做到这一点，我们只需按 1/√(input_dim) 进行重新缩放"
   ],
   "id": "453bb4d2d476bd9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " w = nn.Parameter(torch.randn(input_dim, output_dim) / np.sqrt(input_dim))\n",
    " output = x @ w"
   ],
   "id": "5d5515dff36f435a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "现在输出的每个元素都是常数：-1.5302726030349731。\n",
    "\n",
    "在常数范围内，这就是 Xavier 初始化。\n",
    "\n",
    "为了更加安全，我们将正态分布截断到 [-3, 3] 区间，以避免出现任何异常值的可能。\n",
    "\n"
   ],
   "id": "3738c5028cd47fc3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "w = nn.Parameter(nn.init.trunc_normal_(torch.empty(input_dim, output_dim), std=1 / np.sqrt(input_dim), a=-3, b=3))",
   "id": "757ed570231201f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def custom_model():",
   "id": "562e8c5f9327235b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "让我们使用 nn.Parameter 构建一个简单的深度线性模型。",
   "id": "dba6cc60e20964cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    D = 64  # Dimension\n",
    "    num_layers = 2\n",
    "    model = Cruncher(dim=D, num_layers=num_layers)\n",
    "\n",
    "    param_sizes = [\n",
    "        (name, param.numel())\n",
    "        for name, param in model.state_dict().items()\n",
    "    ]\n",
    "    assert param_sizes == [\n",
    "        (\"layers.0.weight\", D * D),\n",
    "        (\"layers.1.weight\", D * D),\n",
    "        (\"final.weight\", D),\n",
    "    ]\n",
    "    num_parameters = get_num_parameters(model)\n",
    "    assert num_parameters == (D * D) + (D * D) + D"
   ],
   "id": "6efcafa6bab0f6da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "记得将模型移至 GPU。",
   "id": "3ac65ea5bfe1f0dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    device = get_device()\n",
    "    model = model.to(device)"
   ],
   "id": "6cd51cc47a249640",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "在一些数据上运行该模型。",
   "id": "24747ede97443853"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    B = 8  # Batch size\n",
    "    x = torch.randn(B, D, device=device)\n",
    "    y = model(x)\n",
    "    assert y.size() == torch.Size([B])"
   ],
   "id": "ff389c57bfbf7758",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Linear(nn.Module):\n",
    "    \"\"\"Simple linear layer.\"\"\"\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(input_dim, output_dim) / np.sqrt(input_dim))\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x @ self.weight"
   ],
   "id": "67bc5ecd3c4e8422",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Cruncher(nn.Module):\n",
    "    def __init__(self, dim: int, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            Linear(dim, dim)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.final = Linear(dim, 1)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Apply linear layers\n",
    "        B, D = x.size()\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        # Apply final head\n",
    "        x = self.final(x)\n",
    "        assert x.size() == torch.Size([B, 1])\n",
    "        # Remove the last dimension\n",
    "        x = x.squeeze(-1)\n",
    "        assert x.size() == torch.Size([B])\n",
    "        return x"
   ],
   "id": "d873efb4dc6988f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def get_batch(data: np.array, batch_size: int, sequence_length: int, device: str) -> torch.Tensor:\n",
   "id": "e517c3267bb6c7f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "将样本批量大小的随机位置放入数据中。",
   "id": "a20ae604716e5a83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    start_indices = torch.randint(len(data) - sequence_length, (batch_size,))\n",
    "    assert start_indices.size() == torch.Size([batch_size])"
   ],
   "id": "faa083b2e6ca10b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "对数据进行索引。",
   "id": "80d7520827084023"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    x = torch.tensor([data[start:start + sequence_length] for start in start_indices])\n",
    "    assert x.size() == torch.Size([batch_size, sequence_length])"
   ],
   "id": "ab128386c3b9e7f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 固定内存\n",
    "\n",
    "默认情况下，CPU 张量位于分页内存中。我们可以显式地进行固定。"
   ],
   "id": "1436367a607b5b39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    if torch.cuda.is_available():\n",
    "        x = x.pin_memory()"
   ],
   "id": "7507e80e81d89c84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "这使我们能够将 x 从 CPU 异步复制到 GPU。",
   "id": "c29f1d338caefc37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "    x = x.to(device, non_blocking=True)",
   "id": "acf5d65a84087ad6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "这使我们能够并行执行两项操作（此处未执行）：\n",
    "\n",
    "+ 将下一批数据读取到中央处理器（CPU）中\n",
    "\n",
    "+ 在 GPU 上处理 x。\n",
    "\n",
    "[article](https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/)\n",
    "\n",
    "[article](https://gist.github.com/ZijiaLewisLu/eabdca955110833c0ce984d34eb7ff39?permalink_comment_id=3417135)"
   ],
   "id": "b0d76257d642207c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "  return x",
   "id": "b9ae29764485674f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def note_about_randomness():",
   "id": "d5e87584a1aa8e4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "随机性出现在许多地方：参数初始化、 dropout、数据排序等等。\n",
    "\n",
    "为了保证可复现性，我们建议在每次使用随机性时都传入一个不同的随机种子。\n",
    "\n",
    "确定性在调试时特别有用，这样你就能找到程序缺陷了。\n",
    "\n",
    "有三个地方可以设置随机种子，为了安全起见，你应该一次性全部设置好。"
   ],
   "id": "46602b1515c8e76f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    # Torch\n",
    "    seed = 0\n",
    "    torch.manual_seed(seed)\n",
    "    # NumPy\n",
    "    import numpy as np\n",
    "    np.random.seed(seed)\n",
    "    # Python\n",
    "    import random\n",
    "    random.seed(seed)"
   ],
   "id": "e125c0f2cafab79e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def data_loading():",
   "id": "1145ce3b965b0677",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "在语言建模中，数据是一系列整数（由分词器输出）。\n",
    "\n",
    "将它们序列化为 numpy 数组很方便（这由分词器完成）。"
   ],
   "id": "773c2f9b587cd10e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    orig_data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.int32)\n",
    "    orig_data.tofile(\"data.npy\")"
   ],
   "id": "50aa3b18097c0207",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "你可以将它们重新加载为 numpy 数组。\n",
    "\n",
    "不想一次性将全部数据加载到内存中（LLaMA 数据为 2.8TB）\n",
    "\n",
    "使用内存映射（memmap）来延迟加载，只将被访问的部分加载到内存中。"
   ],
   "id": "66e29de6f4ca7c9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    data = np.memmap(\"data.npy\", dtype=np.int32)\n",
    "    assert np.array_equal(data, orig_data)"
   ],
   "id": "a56dd8ec2c8c7813",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "数据加载器会生成一批用于训练的序列。",
   "id": "21351d7f33823af6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    B = 2  # Batch size\n",
    "    L = 4  # Length of sequence\n",
    "    x = get_batch(data, batch_size=B, sequence_length=L, device=get_device())\n",
    "    assert x.size() == torch.Size([B, L])"
   ],
   "id": "e4b70f4e7c9411d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class SGD(torch.optim.Optimizer):\n",
    "    def __init__(self, params: Iterable[nn.Parameter], lr: float = 0.01):\n",
    "        super(SGD, self).__init__(params, dict(lr=lr))\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"]\n",
    "            for p in group[\"params\"]:\n",
    "                grad = p.grad.data\n",
    "                p.data -= lr * grad"
   ],
   "id": "96bc3236a8c7350f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class AdaGrad(torch.optim.Optimizer):\n",
    "    def __init__(self, params: Iterable[nn.Parameter], lr: float = 0.01):\n",
    "        super(AdaGrad, self).__init__(params, dict(lr=lr))\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"]\n",
    "            for p in group[\"params\"]:\n",
    "                # Optimizer state\n",
    "                state = self.state[p]\n",
    "                grad = p.grad.data\n",
    "                # Get squared gradients g2 = sum_{i<t} g_i^2\n",
    "                g2 = state.get(\"g2\", torch.zeros_like(grad))\n",
    "                # Update optimizer state\n",
    "                g2 += torch.square(grad)\n",
    "                state[\"g2\"] = g2\n",
    "                # Update parameters\n",
    "                p.data -= lr * grad / torch.sqrt(g2 + 1e-5)"
   ],
   "id": "951c316684d2086f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def optimizer():",
   "id": "9f7cf9793f21348",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "回想一下我们的深度线性模型。",
   "id": "a1c4d7301af9e952"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    B = 2\n",
    "    D = 4\n",
    "    num_layers = 2\n",
    "    model = Cruncher(dim=D, num_layers=num_layers).to(get_device())"
   ],
   "id": "f437dee2fe07bc1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "让我们来定义 AdaGrad 优化器\n",
    "\n",
    "+ 动量 = 随机梯度下降 + 梯度的指数平均\n",
    "\n",
    "+ AdaGrad = 随机梯度下降 + 按梯度平方进行平均\n",
    "\n",
    "+ RMSProp = AdaGrad + 梯度平方的指数平均\n",
    "\n",
    "+ Adam = 均方根传播（RMSProp） + 动量（momentum）\n",
    "\n",
    "[AdaGrad](https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)"
   ],
   "id": "bf0e0b7379c842dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    optimizer = AdaGrad(model.parameters(), lr=0.01)\n",
    "    state = model.state_dict()  # @inspect state"
   ],
   "id": "929ac2bea9e4ce20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "计算梯度",
   "id": "23e28c550a30acdf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    x = torch.randn(B, D, device=get_device())\n",
    "    y = torch.tensor([4., 5.], device=get_device())\n",
    "    pred_y = model(x)\n",
    "    loss = F.mse_loss(input=pred_y, target=y)\n",
    "    loss.backward()"
   ],
   "id": "6040b43b56001cdf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "迈出一步",
   "id": "b3cfddea8b5b9d6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    optimizer.step()\n",
    "    state = model.state_dict()"
   ],
   "id": "d7223e85e0fd4b18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "释放内存（可选）",
   "id": "aad3a3b45981c0d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "optimizer.zero_grad(set_to_none=True)",
   "id": "3514e1328005570c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 内存",
   "id": "65eb2be848ecab60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    # Parameters\n",
    "    num_parameters = (D * D * num_layers) + D  # @inspect num_parameters\n",
    "    assert num_parameters == get_num_parameters(model)"
   ],
   "id": "7e16e816c53b83ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    # Activations\n",
    "    num_activations = B * D * num_layers  # @inspect num_activations"
   ],
   "id": "18332688305c085b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    # Gradients\n",
    "    num_gradients = num_parameters  # @inspect num_gradients"
   ],
   "id": "7f7202a1a0b6faf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    # Optimizer states\n",
    "    num_optimizer_states = num_parameters  # @inspect num_optimizer_states"
   ],
   "id": "164dd2d024409969",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    # Putting it all together, assuming float32\n",
    "    total_memory = 4 * (num_parameters + num_activations + num_gradients + num_optimizer_states)"
   ],
   "id": "425dbfdc83c33e00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 计算",
   "id": "5d1262c9cff34b97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": " flops = 6 * B * num_parameters",
   "id": "f4ec770ec91e10ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Transformers\n",
    "\n",
    "Transformer 的会计处理更为复杂，但原理是一样的。\n",
    "\n",
    "作业 1 会要求你做那件事。\n",
    "\n",
    "描述 Transformer 训练内存使用情况的博客文章 [文章](https://erees.dev/transformer-memory/)\n",
    "\n",
    "描述 Transformer 的 FLOPs 的博客文章： [文章](https://www.adamcasson.com/posts/transformer-flops)"
   ],
   "id": "5e22fc3b1f80e40b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def train_loop():",
   "id": "fab4beeaf8c572f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "从权重为（0，1，2，……，D-1）的线性函数生成数据。",
   "id": "c8195981872da203"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    D = 16\n",
    "    true_w = torch.arange(D, dtype=torch.float32, device=get_device())\n",
    "    def get_batch(B: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = torch.randn(B, D).to(get_device())\n",
    "        true_y = x @ true_w\n",
    "        return (x, true_y)"
   ],
   "id": "74a1d874dd18eb9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "我们来进行一次基本的运行",
   "id": "48207effceea18df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "    train(\"simple\", get_batch, D=D, num_layers=0, B=4, num_train_steps=10, lr=0.01)",
   "id": "73888888e34991d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "进行一些超参数调优",
   "id": "5625cfb09758ded3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "    train(\"simple\", get_batch, D=D, num_layers=0, B=4, num_train_steps=10, lr=0.1)",
   "id": "e541e0317edd2c90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train(name: str, get_batch,\n",
    "          D: int, num_layers: int,\n",
    "          B: int, num_train_steps: int, lr: float):\n",
    "    model = Cruncher(dim=D, num_layers=0).to(get_device())\n",
    "    optimizer = SGD(model.parameters(), lr=0.01)\n",
    "    for t in range(num_train_steps):\n",
    "        # Get data\n",
    "        x, y = get_batch(B=B)\n",
    "        # Forward (compute loss)\n",
    "        pred_y = model(x)\n",
    "        loss = F.mse_loss(pred_y, y)\n",
    "        # Backward (compute gradients)\n",
    "        loss.backward()\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)"
   ],
   "id": "682cad582c2ea1a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def checkpointing():",
   "id": "d8598b6e8f5fd9a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "训练语言模型需要很长时间，而且肯定会崩溃。\n",
    "\n",
    "你不会想失去所有的进展。\n",
    "\n",
    "在训练过程中，定期将模型和优化器的状态保存到磁盘是很有用的。"
   ],
   "id": "1c5264bcc5194959"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    model = Cruncher(dim=64, num_layers=3).to(get_device())\n",
    "    optimizer = AdaGrad(model.parameters(), lr=0.01)"
   ],
   "id": "8ccf454967cf7a25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "保存检查点：",
   "id": "8cd953bf4074308"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "     checkpoint = {\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, \"model_checkpoint.pt\")"
   ],
   "id": "15008b4b2bdc717b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "加载检查点：",
   "id": "cf7b2099af30a1ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "    loaded_checkpoint = torch.load(\"model_checkpoint.pt\")",
   "id": "49cc308130d4995c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "def mixed_precision_training():",
   "id": "9f10f2fcec24fa5c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "数据类型（float32、bfloat16、fp8）的选择各有取舍。\n",
    "\n",
    "+ 更高的精度：更准确 / 稳定，需要更多内存，需要更多计算资源\n",
    "\n",
    "+ 较低精度：精度 / 稳定性较差，内存占用更少，计算量更少\n",
    "\n",
    "我们怎样才能两全其美呢？\n",
    "\n",
    "解决方案：默认使用 float32，但在可能的情况下使用 {bfloat16, fp8}。\n",
    "\n",
    "一个具体的计划：\n",
    "\n",
    "+ 在前向传播（激活）中使用 {bfloat16, fp8}。\n",
    "\n",
    "+ 其余部分（参数、梯度）使用 float32。\n",
    "\n",
    "+ 混合精度训练[Micikevicius+ 2017]([Micikevicius+ 2017])\n",
    "\n",
    "PyTorch 有一个自动混合精度（AMP）库。\n",
    "https://pytorch.org/docs/stable/amp.html\n",
    "\n",
    "https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/\n",
    "\n",
    "NVIDIA 的 Transformer 引擎支持线性层使用 FP8\n",
    "\n",
    "在整个训练过程中普遍使用 FP8 [Peng+ 2023](https://arxiv.org/pdf/2310.18313.pdf)"
   ],
   "id": "d588fa8611279a79"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "cef1ca3d93e879e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_memory_usage(x: torch.Tensor):\n",
    "    return x.numel() * x.element_size()\n",
    "def get_promised_flop_per_sec(device: str, dtype: torch.dtype) -> float:\n",
    "    \"\"\"Return the peak FLOP/s for `device` operating on `dtype`.\"\"\"\n",
    "    if not torch.cuda.is_available():"
   ],
   "id": "f41dbfa6269017fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "没有可用的 CUDA 设备，因此无法获取每秒浮点运算次数（FLOP/s）。\n",
   "id": "f019e20efbfd2b22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "         return 1\n",
    "    properties = torch.cuda.get_device_properties(device)\n",
    "\n",
    "    if \"A100\" in properties.name:\n",
    "        # https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf\")\n",
    "        if dtype == torch.float32:\n",
    "            return 19.5e12\n",
    "        if dtype in (torch.bfloat16, torch.float16):\n",
    "            return 312e12\n",
    "        raise ValueError(f\"Unknown dtype: {dtype}\")\n",
    "    if \"H100\" in properties.name:\n",
    "        # https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet\")\n",
    "        if dtype == torch.float32:\n",
    "            return 67.5e12\n",
    "        if dtype in (torch.bfloat16, torch.float16):\n",
    "            return 1979e12 / 2  # 1979 is for sparse, dense is half of that\n",
    "        raise ValueError(f\"Unknown dtype: {dtype}\")\n",
    "    raise ValueError(f\"Unknown device: {device}\")"
   ],
   "id": "c081d27afd8107db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d298b82088e3e600"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def same_storage(x: torch.Tensor, y: torch.Tensor):\n",
    "    return x.untyped_storage().data_ptr() == y.untyped_storage().data_ptr()"
   ],
   "id": "8275449013019152",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def time_matmul(a: torch.Tensor, b: torch.Tensor) -> float:\n",
    "    \"\"\"Return the number of seconds required to perform `a @ b`.\"\"\"\n",
    "    # Wait until previous CUDA threads are done\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    def run():\n",
    "        # Perform the operation\n",
    "        a @ b\n",
    "        # Wait until CUDA threads are done\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "    # Time the operation `num_trials` times\n",
    "    num_trials = 5\n",
    "    total_time = timeit.timeit(run, number=num_trials)\n",
    "    return total_time / num_trials"
   ],
   "id": "4925de60c7fdda16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_num_parameters(model: nn.Module) -> int:\n",
    "    return sum(param.numel() for param in model.parameters())"
   ],
   "id": "1705c5fbcbbd4370",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_device(index: int = 0) -> torch.device:\n",
    "    \"\"\"Try to use the GPU if possible, otherwise, use CPU.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(f\"cuda:{index}\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")"
   ],
   "id": "436cccfd8fcc1d9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "e06b4a5c3ac8f9b1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
