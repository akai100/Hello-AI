# 1. 什么是 MoE？


用（多个）大型前馈网络和一个选择器层取代单个大型前馈网络

你可以增加专家数量，而不会影响浮点运算量（FLOPs）

# 为什么专家混合模型（MoEs）正变得流行？

相同浮点运算量下，参数越多（模型）表现越好

