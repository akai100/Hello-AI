# 1. 什么是 MoE？

![](.\imgs\MoE.png)

用（多个）大型前馈网络和一个选择器层取代单个大型前馈网络

你可以增加专家数量，而不会影响浮点运算量（FLOPs）

# 为什么专家混合模型（MoEs）正变得流行？

相同浮点运算量下，参数越多（模型）表现越好

![](.\imgs\MoE popular.png)

训练混合专家模型（MoEs）的速度更快

![](.\imgs\Faster to train MoEs.png)

可并行化到多个设备

![](.\imgs\parallelizable.png)

# 为什么混合专家模型（MoEs）没有更受欢迎呢？

基础设施很复杂 / 多节点的优势

```angular2html
从宏观层面来说，当你拥有大量加速器（例如 GPU/TPU）来承载稀疏化带来的额外参数时，
稀疏性是很有用的。通常模型会采用数据并行的方式训练：不同机器会处理训练 / 推理数据的不同分片。
而原本用于处理这些数据分片的机器，现在可以用来承载更多的模型参数。因此，稀疏模型适用于两种场景：
一是基于数据并行的训练；二是服务时需要高吞吐量的场景 —— 此时多台机器可以共同承载所有参数。
```

训练目标在某种程度上具有启发性（有时还不稳定）

# 混合专家模型（MoEs）通常是什么样子的

典型做法：用 MoE 层替换 MLP

![](.\imgs\2.png)

+ 左侧：稀疏模型的基础模块结构

    稀疏模型的核心层是 Sparse FFN Layer（稀疏前馈层），它被嵌套在残差连接+层归一化之间，同时搭配“自注意力”模块。

+ 右侧：稀疏 FFN 的工作方式（以输入 “The”“ Dog” 为例） 

    输入：两个 token（“The” 对应 x1 、“Dog” 对应 x2）先经过自注意力层处理；

    稀疏 FFN 层：包含多个独立的 FFN 子模块（图中是 FFN 1~4）。

    关键在于：不同 token 会 “激活不同的 FFN 子模块”—— 比如 x1 激活了 FFN 2，x2 激活了 FFN 1；

    输出：经过 “Add + Normalize” 后，得到对应 token 的输出 y1 、 y2


不太常见：用于注意力头的混合专家模型（MoE）

![](.\imgs\3.png)

整个模块包含两层稀疏子结构，每层都遵循 “子模块组 + Router + Layer Norm” 的组合：

+ 上层：4 个MLP（多层感知机）子模块，搭配一个Router，之后接Layer Norm（层归一化）；

+ 下层：4 个Att（注意力）子模块，同样搭配一个Router，之后接Layer Norm；

Router是 “稀疏化” 的关键：它会根据当前输入数据的特征，从对应的子模块组（如上层的 MLP 1~4、下层的 Att 1~4）
中，选择其中一个（或少数几个）子模块来实际参与计算，其余子模块则 “闲置”。

这种结构的核心是用 “大量子模块” 保证模型的参数容量（类似大模型的表达能力），
同时用 “Router 选少数子模块” 减少单样本的计算量—— 既实现了 “大模型的效果”，
又降低了训练 / 推理的资源消耗，是稀疏模型（如 MoE：Mixture of Experts）的典型思路。

# MoE - 参数

+ 路由功能

+ 专家大小

+ 训练目标

##  路由功能 - 概述

许多路由算法归根结底都是 “选择前 k 个”。

## 路由类型

几乎所有的混合专家模型（MoE）都会采用标准的 **“按 Token 选择 Top-K”** 路由策略。近期的部分消融实验：

## 详细解析主流路由变体

## 其他路由方法



