# 大纲与目标

+ 对 “标准” Transformer 的快速回顾（你需要实现的内容）；

  + 大多数大型语言模型（Large LMs）有哪些共同之处？

+ 架构 / 训练过程中常见的变体有哪些？

今日主题：学习最好的方式是亲身体验，其次是借鉴他人的经验。

# 起点：“原始” Transformer

**回顾：** 标准 Transformer 中的设计选择

**位置编码：** sines（正弦）和 cosine（余弦）

 $PE_{pos,2i}=sin(pos/10000^{2i/d_{model}})$

 $PE_{pos,2i+1}=cos(pos/10000^{2i/d_{model}})$

 **FFN(前馈神经网络)：** ReLu

 $FFN(x)=max(0,xW_{1}+b_{1})W_{2}+b_{2}$

**归一化类型：** 后置归一化，层归一化

# 你所实现的内容 —— 简洁的现代变体

**差异**

+ **层归一化（LayerNorm）** 置于模块（block）之前

+ **旋转位置编码（RoPE）**

+ 前馈层（FF layers）采用 **SwiGLU** 而非 ReLU
 
+ 线性层（以及层归一化）不含偏置（常数）项

# 我们来看一下（关于密集型架构的）相关数据

借鉴现有众多其他模型（及相关论文）的经验

**我们将详细探讨众多主流的架构变体与超参数变体。**

+ 这些模型有哪些共同之处？

+  哪些部分存在差异？

+  我们能从这些（差异与共性）中得到什么启示？

# 我们接下来要涵盖哪些内容？

**常见架构变体**

+ 激活函数（Activations）与前馈网络（FFN）

+ 注意力机制变体

+ 位置编码

**重要 / 不重要的超参数**

+ 什么是 ff_dim？多头维度的总和是否始终等于模型维度？

+ 词汇表包含多少个元素？

**稳定性优化技巧**

# 架构变体

我们来聚焦核心架构组件

**高层视角**

+  低共识度 *（指行业内对某类设计方案缺乏统一认可）* （Pre-Norm 除外）

+ 趋向 “类 LLaMA 架构 *（LLaMA 是 Meta 推出的开源大模型系列）*” 的发展趋势


# 前归一化 VS 后归一化

Post-LN Transformer

 $x_{l,i}^{post,1}=MultiHeadAtt(x_{l,i}^{post},[x_{l,1}^{post},...,x_{l,n}^{post}])$

 $x_{l,i}^{post,2}=x_{l,i}^{post}+x_{l,i}^{post,1}$

 $x_{l,i}^{post,3}=LayerNorm(x_{l,i}^{post,2})$

 $x_{l,i}^{post,4}=ReLU(x_{l,i}^{post,3}W^{1,l}+b^{1,l})W^{2,l}+b^{2,l}$

 $x_{l,i}^{post,5}=x_{l,i}^{post,3}+x_{l,i}^{post,4}$
 
 $x_{l+1,i}^{post}=LayerNorm(x_{l,i}^{post,5})$

 Pre-LN Transformer

 $x_{l,i}^{pre,1}=LayerNorm(x_{l,i}^{pre})$

 $x_{l,i}^{pre,2}=MultiHeadAtt(x_{l,i}^{pre,1},[x_{l,1}^{pre,1},...,x_{l,n}^{pre,1}])$

 $x_{l,i}^{pre,3}=x_{l,i}^{pre}+x_{l,i}^{pre,2}$

 $x_{l,i}^{pre,4}=LayerNorm(x_{l,i}^{pre,3}$

 $x_{l,i}^{pre,5}=ReLU(x_{l,i}^{pre,4}W^{1,l}+b^{1,l})W^{2,l}+b^{2,l}$

 $x_{l+1,i}^{pre,1}=x_{l,i}^{pre,5}+x_{l,i}^{pre,3}=$

 Final LayerNorm: $x_{Final,i}^{pre} \rightrows LayerNorm(x_{L+1}^{pre},i)$

  配置 LayerNorm 使其不影响左侧的主残差信号路径

**几乎所有现代语言模型（LMs）都采用前归一化（Pre-Norm）（但 BERT 使用的是后归一化（Post-Norm））**

# 前归一化与后归一化的对比数据

# 前归一化与后归一化（的对比），请解释说明？

**最初宣称的优势**是移除学习率预热（warmup）*（指训练初期逐步提升学习率的策略）*。**如今**（其核心价值）则是为大型网络提供训练稳定性，并支持更大的学习率（LRs）。


# 新趋势 / 新方案 ——“双重归一化

既然将 LayerNorm 放入残差流中是有害的…… 那为何不把后归一化（Post-Norm）置于残差流之外呢？
图
近期模型：Grok、Gemma 2（均采用双重归一化）。Olmo 2 则仅使用非残差后归一化（non-residual post norm）。

# 层归一化（LayerNorm）与均方根归一化（RMSNorm）

原始 Transformer（模型）：采用层归一化（LayerNorm）—— 对模型维度（ $d_{model}$ₗ）上的均值和方差进行归一化。

 $y=\frac{x-E[x]}{\sqrt{Var[x]+e}*\gamma+\belta}$

许多现代语言模型（LMs）：采用**均方根归一化（RMSNorm）**—— 不减去均值（去中心化），也不添加偏置项。

 $y=\frac_{x}{\sqrt{||x||_{2}^{2}+e}}*\gamma$

# 为什么选择 均方根归一化（RMSNorm）

**如今的解释**是：它（RMSNorm）速度更快（且效果不相上下）。

+ 运算步骤更少（无需计算均值）

+ 参数数量更少（无需存储偏置项）

解释是否合理


**重要结论**：浮点运算次数（FLOPS）不等于实际运行时间！

# RMSNorm - 验证

多篇论文已证实，均方根归一化（RMSNorm）能带来运行时间（ runtime ）的提升，且意外地实现了模型性能（ perf ）的优化。


# 更普遍的情况是：移除偏置项

**大多数现代 Transformer（模型）已不再设置偏置项**

原始 Transformer:

$FFN(x)=max(0,xW_{1}+b_{1})W_{2}+b_{2}$

大多数实现方案（若未采用门控机制）：

 $FFN(x)=𝜎(xW_{1})W_{2}$

**原因：**内存占用（与 RMSnorm 类似）和优化稳定性

# LayerNorm：回顾

+ **基本上所有人都采用预归一化（pre-norm）**

  + 核心思路 —— 保留残差连接的优势
  
  + 实际效果 —— 梯度传播更顺畅，尖峰现象更少

  + 部分研究者会在残差流外部额外添加一层归一化（并非后归一化 post-norm）


+ **大多数人采用 RMS 归一化（RMSnorm）**

  + 实际应用中，其效果与层归一化（LayerNorm）相当

  + 但它需要处理的参数更少，因此能节省实际运行时间

  + 由于计算成本与参数数量的权衡性价比不高，研究者通常会舍弃偏置项（bias terms）

# 激活函数

**各式各样的激活函数**

RLU，GeLU, Swish，ELU, GLU，GeGLU，ReGLU, SeLU, SwiGLU, LiGLU

**这些都是什么？人们实际用哪些？这（选择）重要吗？**

# 一些常见的激活函数

**RELU**

 $FF(x)=max(0, xW_{1})W_{2}$

 **GeLU**

 $FF(x)=GELU(xW_{1})W_{2}$

 $GELU(x):=xΦ(x)$

**swiGLU/GeGLU**

# 门控激活函数（*GLU 类）

GLU（门控线性单元）会修改前馈层（FF 层）的 “第一部分”

 $FF(x)=max(0,xW_{1})W_{2}$

不再使用 “线性变换 + ReLU” 的组合，而是在上述结构基础上增加一个（按元素的）线性项

 $max(0,xW_{1})→max(0,xW_{1})⊗(xV)$

这就形成了门控变体（ReGLU）—— 需要注意的是，这种结构会引入一个额外的参数（V）

 $FF_{ReGLU}(x)=(max(0,xW_{1})⊗xV)W_{2}$

# 标准前馈层（FF 层）的门控变体

**GeGLU**

$FFN_{GEGLU}(x,W,V,W_{2})=(GELU(xW)⊗xV)W_{2}$

**SwiGLU**(swish 是 $x * sigmoid(x)$)

 $FFN_{SwiGLU}(x,W,V,W_{2})=(Swish_{1}(xW)⊗xV)W_{2}$

 注意：门控模型会将前馈层维度（ $d_{ff}$）按 2/3 的比例缩小（即仅为原维度的 2/3）

 # 门控线性单元（GLU）真的有效吗？

 是的，效果相当稳定。

 # 门控（Gating）、激活函数（activations）

+ 不同模型中存在诸多变体（ReLU、GeLU、*GLU 类）

+ *GLU 类（门控线性单元变体）并非构建优秀模型的必需组件（参见 GPT-3），但它大概率能起到辅助作用
  
  另外，近年来还出现了一些非主流模型案例 ——Nemotron 340B（采用平方 ReLU 激活函数）、Falcon 2 11B（采用 ReLU 激活函数）

+ 但现有证据表明，SwiGLU/GeGLU（注：原文 “Swi/GeGLU” 为缩写）能带来相对稳定的性能提升

# 串行层与并行层

标准 Transformer 块采用串行结构 —— 先计算注意力（机制），再执行多层感知机（MLP）

我们能否将 Transformer 块并行化？

# 并行层

有部分模型（如 GPT-J、PaLM、GPT-NeoX）采用了并行层结构，这一设计最初源于 GPT-J。

并行层 —— 我们在每个 Transformer 块中采用了 “并行” 结构（Wang & Komatsuzaki, 2021），而非标准的 “串行” 结构。具体而言，标准结构可表述为：

 $y=x+MLP(LayerNorm(x+Attention(LayerNorm(x)))$

 而并行结构则可表述为：

 $y=x+MLP(LayerNorm(x))+Attention(LayerNorm(x))$

并行结构在大规模场景下可使训练速度提升约 15%，这是因为多层感知机（MLP）与注意力机制的输入矩阵乘法能够被融合执行。消融实验显示，在 80 亿参数规模下模型性能出现小幅下降，但在 620 亿参数规模下无性能损失；因此我们推断，并行层在 5400 亿参数规模下应不会对性能产生影响（即性能中性）。

若实现得当，层归一化（LayerNorm）可以共享，且矩阵乘法能够被融合执行

近期主流模型：Cohere Command A、Falcon 2 11B、Command R+

# 架构总结

**前置VS后置归一化**

+ 所有模型均采用前置归一化（pre-norm）（OPT-350M 除外），这一设计选择很可能有充分的理由。

**层归一化（LayerNorm）与均方根归一化（RMSNorm）**

+ 均方根归一化（RMSNorm）在计算效率上优势显著，有时甚至能带来性能提升

**门控**

+ 门控线性单元（GLU）类激活函数整体表现似乎更优，尽管优势差异并不显著

**串行/并行层**

+ 尽管尚未开展极为严谨的消融实验验证其效果，但（该方案）确实具备计算效率上的优势

# 位置编码（position embeddings）存在诸多变体

**正弦编码（Sine embeddings）：** 添加正弦与余弦函数值以实现位置定位

 $Embed(x,i)=v_{x}+PE_{pos}$
 
 $PE_{pos,2i}=sin(pos/10000^{2i/d_{model}})$

 $PE_{pos,2i+1}=cos(pos/10000^{2i/d_{model}})$

**绝对编码（Absolute embeddings）**：为词嵌入添加一个位置向量

 $Embed(x,i)=v_{s}+u_{i}$

 **相对编码（Relative embeddings）**：在注意力计算过程中加入（位置相关的）向量

 $e_{ij}=\frac{x_{i}W^{Q}(x_{j}W^{K}+a_{ij}^{K})^T}{\sqrt(d_{z})}$

**RoPE 编码**（Rotary Position Embeddings，旋转位置编码）

# RoPE :Rotary Position Embeddings，旋转位置编码

高层级思考过程：相对位置嵌入（relative position embedding）应当是某种满足特定条件的函数 𝑓(𝑥, 𝑖)，其中：

$<f(x,i),f(y,j)>=g(x,y,i-j)$

 也就是说，注意力函数应当仅依赖于相对位置 (i-j)。现有嵌入方式为何无法实现这一目标？

+ **正弦（嵌入）**：存在多种非相对的交叉项

+ **绝对（嵌入）**：显然不具备相对性

+ **相对嵌入**： $e_{ij}=\frac{x_{i}W^Q(x_jW^K+a_{ij}^K})^T}{\sqrt{d_z}}$ 不是内积


**怎么解决这个问题？**

+ 我们希望我们的嵌入方式对绝对位置具有不变性

+ 我们知道，内积对于任意旋转具有不变性


**有多种旋转，选择哪一个？**

#  旋转位置编码（RoPE）的核心数学原理

与正弦和余弦相乘

$f_{q,k}(x_m,m)=R_{}$

# RoPE 实现代码


# 超参

你在 224n 课程中可能遇到的 Transformer 超参数相关问题，

+ 前馈网络维度应当比隐藏层维度大多少倍？

+ 注意力头数应设为多少？且头数（num_heads）是否必须能整除隐藏层维度（hidden size）？

+ 词汇表大小（vocab size）应设为多少？

以及其他模型设置相关问题

+ 人们真的会对这些超大规模语言模型（LM）进行正则化吗？

+ 人们是如何扩展这些模型的 —— 是追求极深的网络层数，还是极宽的模型维度？


# （令人意外的？）首个共识超参数

前馈网络维度与模型维度的比值（简称：前馈 - 模型维度比）

 $FFN(x)=max(0,xW_1+b_1)W_2+b_2$

存在两个相关维度 —— 前馈网络维度（𝑑𝑓𝑓）与模型维度（𝑑𝑚𝑜𝑑𝑒𝑙）。它们之间应保持何种关系？

$d_{ff}=4d_{model}$

这一点几乎总是成立的，仅存在少数例外情况

# 例外情况 1—— 门控线性单元（GLU）变体

要注意的是，门控线性单元（GLU）变体的维度会按 $2/3^{rd}$ 的比例缩减。这意味着大多数 GLU 变体的前馈网络维度满足 $𝑑_{𝑓𝑓} = \frac{8}{3}𝑑_{𝑚𝑜𝑑𝑒𝑙}$ 这是最常见的情况，以下是一些典型案例。

# 例外情况 2—— T5

正如我们已经（并将继续）看到的，大多数语言模型（LMs）都采用乏味、保守的超参数设置。
一个例外是 T5 模型 [Raffel 等人，2020]，它采用了一些非常大胆的设置。

具体来说，对于 110 亿参数的模型（11B model），他们设定了

$d_{ff}=65536$
$d_{model}=1024$

达到了惊人的 64 倍乘数（比例）。

其他近期的例外案例包括 ——Gemma 2（8 倍乘数）、SmolLM/Gemma 3（4 倍乘数，采用门控线性单元 GLU）。

# 为何是这一范围的乘数？

从实验数据来看，在 1 到 10 的区间内存在一个 “最优域”（basin），此时该超参数的表现接近最优。

# 我们能从模型维度（model-dim）这一超参数中得到哪些启示？

+ 将前馈网络维度（ $d_{ff}$）设为模型维度（ $d_{model}$）的4倍（ $d_{ff}=4d_{model}$）或 2.66 倍（ $d_{ff}=2.66d_{model}$）的默认选择，已在几乎所有现在大语言模型（LLMs）中被证实非常有效;

+ 但 T5 模型的确证明，即便是将 $d_{ff}$ 设为 $d_{model}$ 的 64 倍这种激进选择，也能奏效。这一超参数的选择并非一成不变（并非板上钉钉）;

+ 尽管如此，T5 拥有一个经过 “改进” 的后续模型（T5 v1.1）—— 它在 GeGLU 结构上采用了更为标准的 2.5 倍乘数，因此（原先的）64 倍乘数很可能并非最优选择；


# 令人惊讶（或许并不意外？）的共识性超参数 2：

注意力头维度（Head-dim）× 注意力头数（num-heads）与模型维度（model-dim）的比值。补充说明：相关内容可参考 224n 的幻灯片（slide）

但这并非绝对成立：我们完全可以让注意力头维度（head-dimensions）大于模型维度除以注意力头数的结果（model-dim /num-heads）。

# 该设置下的注意力头数是多少？模型维度又是多少？
 

# 支持 “1:1 比例” 的依据是什么？

已有多篇论文反驳 1:1 比例的合理性 [Bhojanapalli 等人，2020]。

但在实际应用中，我们似乎并未观察到明显的 “低秩瓶颈” 现象。

# 宽高比 / 比例系数

我的模型应当往深度方向发展还是宽度方向发展？具体该设计多深、多宽？

多数模型在这一点上的一致性也出人意料！

# 关于比例系数的考量因素

**极深的模型不仅更难实现并行化，还会带来更高的延迟。**

**深度与宽度的局限性** 我们需指出，我们的建议存在一个明显的局限性：深度扩展存在显著制约 —— 即模型深度无法跨不同机器或设备实现并行化，且每一步计算都必须等待前一层完成。这与宽度扩展形成鲜明对比：
宽度可轻松在数千甚至数十万台设备上实现并行化。在扩展的局限性框架内……


# 比例系数缩放的实证依据

典型的词汇量大小（vocabulary sizes）是多少？

# Dropout（随机失活）及其他正则化方法

# 实际应用中的 Dropout（随机失活）与权重衰减

# 大语言模型（LLMs）中为何要使用权重衰减（weight decay）？

[Andriushchenko 等人，2023] 的研究针对大语言模型（LLM）的权重衰减提出了颇具价值的发现。

# 总结: 超参

**前馈**
+ 经验性的 4 倍法则（GLU 类结构为 8/3 倍）已是行业标准（且有相关实证依据支撑）

**头维度**

+ “注意力头维度 × 头数 = 模型维度” 已是行业标准配置

**比例系数**

+ 存在一个宽泛的 “有效” 取值范围（100-200），最终取值由系统层面的考量决定

**正则化**

+ 你仍需对语言模型（LMs）进行正则化，但正则化的作用主要体现在优化动态上


# 稳定性优化技巧

# 问题出在哪里？警惕 Softmax 层！

**Softmax 层** —— 可能因指数运算 / 除以零问题表现失常

# 输出层 Softmax 的稳定性优化 ——“z-loss”（Z 损失）

回顾下 Softmax 的核心计算过程

 $log(P(x))=log(\frac{e^{U_r(x)}}{Z(x)})$
 $=U_r(x)-log(Z(x))$
 $Z(x)=\sum_{r'=1}{|V|}{e^{U_{r'}(x)}}$

 # 注意力机制中 Softmax 的稳定性优化 ——“QK 归一化（QK norm）”

 # Logit 软截断

 通过 Tanh 函数将 logit 值软截断至某一最大值

# 注意力头（Attention heads）

# GQA/MQA—— 降低注意力头的成本

# MQA（多查询注意力）—— 本质是仅保留更少的键（K）/ 值（V）维度

# 近期衍生方案 —— 分组查询注意力（GQA）

# 多查询注意力（MQA）会造成负面影响吗？有时会

# 稀疏注意力（Sparse Attention）/ 滑动窗口注意力（Sliding Window Attention）

# 当前的标准技巧 —— 交错使用 “全注意力（full attention）” 与 “低秩注意力（LR attention）”


