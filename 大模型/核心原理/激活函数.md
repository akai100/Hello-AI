
## 1. GELU

GELU(Gaussian Error Linear Units) 是当前大模型中主流的激活函数，本质是**融合了高斯分布的平滑非线性激活**，相比 ReLU 更贴合自然语言处理/深度学习的特征分布。

### 1.1 核心定义与数学逻辑

**1. 原始数学公式**

 $GELU(x) = x \dot \phi(x)$

 其中 $\phi(x)$ 是标准正态分布的 CDF，即：

 $\phi(x)=\frac{1}{2}[1+erf(\frac{x}{\sqrt{2}})]$

+ $erf$ 是误差函数，描述正态分布的概率累积；

直观理解：GELU 是 “带概率的 ReLU” ——— 对每个输入 $x$，以 $\phi(x)$ 的概率保留 $x$，以 $1-\phi(x)$ 的概率置0（相比 ReLU 的硬阈值，GELU 是平滑的软阈值）

**2. 近似简化公式（工程实现）**

原始公式涉及 $erf$ 计算，效率低下，工程上用近似版：

 $GELU(x) \approx 0.5x \dot [1 + tanh(\sqrt{\frac{2}{\pi}}(x+0.044715x^3))]$

或更简单的近似：

 $GELU(x) \approx x \dot \sigma(1.702x)$ 

## 2. SwiGLU

## 3. ReLU

## 4. Mish
