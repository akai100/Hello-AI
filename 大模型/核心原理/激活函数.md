
## 1. GELU

GELU(Gaussian Error Linear Units) 是当前大模型中主流的激活函数，本质是**融合了高斯分布的平滑非线性激活**，相比 ReLU 更贴合自然语言处理/深度学习的特征分布。

### 1.1 核心定义与数学逻辑

**1. 原始数学公式**

 $GELU(x) = x \dot \phi(x)$

 其中 $\phi(x)$ 是标准正态分布的 CDF，即：

 $\phi(x)=\frac{1}{2}[1+erf(\frac{x}{\sqrt{2}})]$

+ $erf$ 是误差函数，描述正态分布的概率累积；

直观理解：GELU 是 “带概率的 ReLU” ——— 对每个输入 $x$，以 $\phi(x)$ 的概率保留 $x$，以 $1-\phi(x)$ 的概率置0（相比 ReLU 的硬阈值，GELU 是平滑的软阈值）

**2. 近似简化公式（工程实现）**

原始公式涉及 $erf$ 计算，效率低下，工程上用近似版：

 $GELU(x) \approx 0.5x \dot [1 + tanh(\sqrt{\frac{2}{\pi}}(x+0.044715x^3))]$

或更简单的近似：

 $GELU(x) \approx x \dot \sigma(1.702x)$ 

### 1.2 核心特性

**1. 平滑的非线性**

ReLU 是“硬非线性”（ $x < 0$ 时梯度为0），而 GELU 在整个实数域连续可导，梯度平滑，避免 ReLU 的 “死亡神经元” 问题，提升训练3稳定性。

**2. 概率加权特性**

相比 Swish（ $x \cdot \sigma(x)$），GELU 的加权系数（高斯 CDF）更贴合自然语言的统计分布，在 Transformer 的自注意力层中能更好地捕捉语义特征。

**3. 无边界输出**

输出范围为 $(-\infty, +\infty)$，相比 ReLU（非负输出），能保留更多负向信息，适配大模型的高维特征空间。

**4. 计算开销适中**

虽略高于 ReLU，但通过近似公式可将计算简化为 “乘法 + sigmoid/tanh”，GPU 上可高效并行，且收益远大于开销（大模型训练收敛更快、精度更高）。

## 2. SwiGLU

## 3. ReLU

## 4. Mish
