
未来掩码（Future Mask/Look-Ahead Mask）是 Transformer Decoder 自注意力层的核心机制，核心作用是强制模型在生成序列时 “只能看到前文，看不到后文”，
符合文本生成、机器翻译等任务的 “自回归（Autoregressive）” 特性 —— 比如生成第 i 个 token 时，只能利用第 1~i-1 个 token 的信息，避免模型 “作弊” 提前看到未来的 token。

## 1. 为什么需要未来掩码

Transformer 的自注意力层默认会计算当前 token 与所有 token（包括前文 + 后文） 的注意力权重，而 Decoder 的核心任务是 “逐词生成序列”（如从左到右生成文本），若不限制对后文的关注：

+ 模型会利用未来 token 的信息生成当前 token，失去 “预测” 的意义；

+ 训练和推理的逻辑不一致（推理时无未来 token），导致模型泛华性极差；

**本质：**

构造一个上三角掩码矩阵，将 “当前 token 对未来 token” 的注意力分数置为负无穷（-∞），经过 softmax 后权重趋近于 0，从而完全忽略未来 token。
