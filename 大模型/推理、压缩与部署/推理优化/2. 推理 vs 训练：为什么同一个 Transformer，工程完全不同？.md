🎯 本模块目标

你要做到：

+ 不是背概念，而是能用“系统 / GPU / 性能”的语言解释差异

## 1️⃣ 表面相同，本质完全不同

从数学上看：

+ 训练：Transformer

+ 推理：Transformer

但从**工程视角：**

| 维度   | 训练          | 推理           |
| ---- | ----------- | ------------ |
| 输入   | 固定长度 batch  | 动态长度请求       |
| 计算模式 | 一次性         | 自回归（逐 token） |
| 并行度  | 极高          | 很低（decode）   |
| 显存占用 | Activations | **KV Cache** |
| 优化目标 | 吞吐          | 延迟 + 吞吐      |

📌 面试金句：

“训练是 batch-first，推理是 latency-first。”

## 2️⃣ 训练阶段：GPU 的「理想工作状态」

**特点**

+ 大 batch

+ 长序列

+ 固定 shape

+ 重计算 OK（反向传播）

👉 结果：

+ Tensor Core 拉满

+ Compute bound

+ GPU 利用率接近 100%

📌 CUDA 视角：

+ kernel 大、并行度高、访存规则

## 3️⃣ 推理阶段：GPU 的「噩梦模式」

**Prefill（还行）**

+ 一次处理多个 token

+ 并行度尚可

**Decode（问题核心）**

+ 每次 1 token

+ 严格串行

+ kernel 很小

+ GPU 经常空转

📌 面试官爱问：

“为什么 decode 阶段 GPU 利用率很低？”

## 4️⃣ 显存使用：最本质的差异

**训练用什么显存？**

+ Activations
+ Gradients
+ Optimizer states

👉 可丢、可重算

**推理用什么显存？**

+ KV Cache
+ 模型权重

**👉 不能丢**

📌 面试金句：

+ “训练阶段可以用算力换显存，推理阶段只能用显存换时间。”

## 5️⃣ Shape & 动态性（工程难点）

**训练**

+ batch size 固定
+ seq len 固定
+ kernel 可提前编译

**推理**

+ 请求长度不一致
+ batch 动态变化
+ kernel 调度复杂

👉 这就是为什么：

+ vLLM 要 Continuous Batching

+ TensorRT 推理难但快

## 6️⃣ 优化目标完全不同

**训练优化**

+ 吞吐最大化
+ scaling
+ 分布式

**推理优化**

+ P99 latency
+ 显存效率
+ 并发请求

📌 面试常考：

+ “如果让你优化推理，你会关注哪些指标？”

## 7️⃣ 一个「大厂级总结模板」

你可以在面试中这样收尾：

+ “虽然训练和推理使用的是同一套 Transformer 结构，但二者在工程上完全不同。

+ 训练阶段是大 batch、固定 shape、计算密集型任务，GPU 主要受算力限制；
+ 而推理阶段尤其是 decode，是高度动态的自回归过程，
+ 并行度低、显存访问频繁，性能更容易受显存带宽和调度策略影响。
+ 因此，推理系统的核心优化方向是提升 decode 阶段 GPU 利用率，并降低 KV Cache 的显存压力。”

## 8️⃣ 本模块作业（这题=大厂面试原题）

**作业（必做）**

如果让你设计一个 LLM 推理系统，为什么不能直接复用训练时的实现？

要求：

5–8 行

必须从 GPU / 显存 / 并行性 角度回答

不要从“业务不同”这种表面原因说

```
训练阶段通常使用大批量数据，输入 shape 固定，整体是计算密集型任务，
GPU 可以充分并行，主要受算力限制。
而推理阶段是自回归过程，每一步只生成一个 token，
并行度明显受限，尤其在 decode 阶段 GPU 利用率较低。
同时，推理需要频繁访问历史 KV Cache，
性能更容易受到显存带宽而不是算力的影响。
因此，训练阶段针对大 batch 和计算吞吐的实现，
无法直接满足推理阶段对低延迟和显存效率的要求。
```
