🎯 本模块目标

你要做到：

**能完整解释：vLLM 为什么吞吐高、延迟稳、显存友好**

## 1. 为什么“普通 batching”在推理中不行？

**传统 batching（训练思路）**

+ 一次 batch 固定
+ 所有请求：
  + 同时开始
  + 同时结束
+ shape 固定

👉 适合训练，不适合推理

**推理场景的真实情况**

+ 请求随时进来
+ 每个请求生成长度不同
+ 有的 10 tokens，有的 1000 tokens

📌 面试金句：

+ “推理是一个 online、异构、动态负载的问题。”

## 2. 静态 batching 的三大问题

**静态 batching = batch 在开始时就固定**

典型流程：

```
1. 收集 N 个请求
2. padding 到相同长度
3. 一次性送入 GPU
4. 所有请求跑完 → batch 结束
```

**① GPU 空转严重**

+ 短请求结束

+ 长请求还在跑

+ GPU 等待

静态 batch 假设所有请求生命周期一致，但推理中完全不是。

**② 延迟极不稳定**

+ 新请求只能等 batch 结束
  
+ tail latency 爆炸

**③ KV Cache 管理困难**

+ batch 边界 = 显存边界

+ 不灵活

静态 batch 要求：所有请求 shape 一致，通常只能：按最大可能长度分配 KV

bacth = 显存生命周期：batch 开始 → 分配 KV， batch 结束 → 释放 KV，👉 无法：单个请求提前释放，动态扩展

## 3. vLLM 的核心：Continuous Batching

**一句话核心思想**

**batch 不是一次性组好，而是在 decode 过程中动态变化的**

**Continuous Batching 在干什么？**

在 **每一个 decode step：**

+ 移除已经完成的请求

+ 插入新的请求

+ batch size 动态变化

📌 面试官爱听：

“Continuous batching 把 batch 变成了一个流。”

## 4. Scheduler 的角色（系统核心）

vLLM 的 Scheduler 负责：

1️⃣ 决定哪些请求进 batch

2️⃣ 分配 KV Cache block（PagedAttention）

3️⃣ 控制 decode step 的执行顺序

**👉 Scheduler = 推理系统的大脑**

## 5. decode 阶段的真实执行流程（一步一步）

每一步 decode：

1. Scheduler 选出 active requests

2. 为新 token 分配 KV block

3. 组织 attention kernel（Flash + Paged）

4. GPU 执行 1 step

5. 更新状态

6. 回到 Scheduler

📌 面试金句：

+ “decode 本质是 scheduler 驱动的一步步状态机。”

## 6. 为什么 Continuous Batching 能提高吞吐？

关键原因（必背）

1️⃣ GPU 永远有活干

2️⃣ batch size 尽量大

3️⃣ 显存利用率高

4️⃣ KV Cache 生命周期可管理

**👉 不是单点快，是系统层快**

## 7. 延迟为什么还能控制住？

这是很多人误解的点。

**vLLM 的设计取舍**

+ 每 step 都允许新请求进入

+ 不等整个 batch 结束

📌 面试金句：

+ “vLLM 用 step 级调度，把吞吐和延迟解耦。”

## 8. 一个「大厂级总结模板」

你可以这样回答：

```
“vLLM 的高性能来自系统级设计。
它通过 Continuous Batching 在 decode 过程中动态调整 batch，
配合 Scheduler 进行请求调度和 KV Cache 分配，
使 GPU 始终保持高利用率。
同时结合 FlashAttention 减少显存访问，
PagedAttention 提升显存利用率，
在保证延迟可控的前提下显著提升整体吞吐。”
```

## 9. 本模块作业（非常关键）

**作业（必做）**

Continuous Batching 相比静态 batching，
为什么既能提升吞吐，又不会显著恶化延迟？

要求：

+ 必须出现：
  + step 级调度
  + 新请求插入
  + GPU 利用率

```
Continuous Batching 通过 step 级调度在 decode 过程中动态调整 batch，
每个 decode step 都可以移除已完成请求并插入新的请求，
从而避免 batch 规模随时间缩小，保证 GPU 始终保持较高利用率，
因此显著提升整体吞吐。

同时，新请求不需要等待整个 batch 结束，
而是在下一个 decode step 即可进入执行，
将等待时间限制在单个 step 的粒度内，
从而在提升吞吐的同时不会显著恶化请求延迟。
```
