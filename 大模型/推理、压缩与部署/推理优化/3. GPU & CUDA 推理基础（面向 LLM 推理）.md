🎯 本模块目标

让你在大厂面试中：

一开口就是 GPU 性能工程师，而不是“会用框架的人”

总览（你在学什么）

你这一模块要解决 3 个核心问题：

1️⃣ GPU 是怎么“干活”的（和 CPU 根本不同）

2️⃣ 为什么 LLM 推理，尤其是 Attention，**天生容易 memory bound**

3️⃣ CUDA kernel 在推理中，**到底慢在哪里**

## 1. GPU 性能的第一性原理

**GPU 不是“快 CPU”**

GPU 的设计目标只有一句话：

+ 用极高的并行度，隐藏内存访问延迟

**三个关键硬件点（面试必考）**

+ SM（Streaming Multiprocessor）
+ Warp（32 个线程）
+ HBM（显存，带宽大但延迟高）

📌 面试金句：
+ “GPU 不是靠低延迟，而是靠并行把延迟摊平。”


## 2. Compute Bound vs Memory Bound（核心）

这是**你之后所有推理优化的判断标准。**

**Compute Bound**

+ 算力打满
+ Tensor Core 忙
+ 算子复杂

📌 典型：训练阶段 GEMM

**Memory Bound**

+ 算力没满
+ HBM 带宽打满
+ 数据搬得慢

📌 典型：推理阶段 Attention

判断方法（工程师思维）
+ 如果算子性能随 FLOPS 提升不明显，但随带宽变化明显 → memory bound

## 3. Attention 为什么天然 memory bound？（重中之重）

我们拆开 Attention 在 **decode** 阶段的行为：

**每生成 1 个 token，要做什么？**

+ 读取：
  + 所有历史 K
  + 所有历史 V
+ 计算：
  + QKᵀ
  + softmax
  + 加权 V
⚠️ 注意：

+ K/V 体积巨大

+ 但 计算量随 token 线性增长

👉 结果：

+ 算得不多
+ 读得很多

📌 面试金句：

+ “Decode attention 是典型的高内存访问、低计算密度算子。”

## 4. 一个直观对比（非常重要）

假设：

+ hidden size = 4096

+ seq len = 2048

**Attention decode**

+ FLOPs：~ O(d)
+ Memory：~ O(seq_len × d)

**👉 算术强度极低** → memory bound

## 5. CUDA kernel 在推理中为什么“小而慢”？

**Decode 阶段 kernel 特点**

+ 每次只处理 1 个 token
+ 启动 kernel 开销占比高
+ 并行线程数少
+ SM 利用率低

📌 面试官爱听：

+ “Decode 阶段不是 kernel 不够快，而是 kernel 太小、太零散。”

## 6. 推理优化的第一原则（非常重要）

你现在可以理解所有推理优化方案：

**想办法减少 HBM 访问，或让一次访问做更多有用的计算**

**所以才有：**

+ KV Cache
+ FlashAttention
+ PagedAttention
+ Continuous Batching

## 7. 一个「大厂级总结」

你可以在面试中这样说：

```
“从 GPU 性能角度看，大模型推理尤其是 decode 阶段，本质是一个 memory bound 问题。
Attention 需要频繁访问历史 KV Cache，算术强度低，
导致 GPU 算力难以充分利用。
因此推理优化的核心不是单纯提高 FLOPS，
而是减少显存访问次数、提高数据复用率和并行度。”
```

## 8. 本模块作业（面试级）

**作业 1（必做）**

为什么 attention 在 decode 阶段比 GEMM 更容易成为 memory bound？

要求：

+ 不要只说“读得多”

+ 必须提到 算术强度 / 并行度 / 访问模式

```
在 decode 阶段，attention 每一步只针对一个新 token 进行计算，
并行度受限，无法充分展开线程。
同时，attention 需要频繁从显存中读取所有历史 KV Cache，
数据访问量随序列长度线性增长，而计算量增长相对较慢，
导致算术强度较低。
在这种访问模式下，GPU 的算力难以被充分利用，
性能主要受显存带宽而非 FLOPS 限制，
因此 decode 阶段的 attention 更容易成为 memory bound。
```

**作业 2（选做，加分）**

如果 GPU 显存带宽翻倍，但 FLOPS 不变，

decode 阶段 attention 的性能会如何变化？为什么？

```
如果 GPU 显存带宽翻倍，但 FLOPS 不变，
decode 阶段 attention 的性能会有一定提升，但不会线性增长。
一方面，decode 阶段是逐 token 的自回归过程，并行度受限，
kernel 规模较小，GPU 难以同时调度足够多的线程来充分利用新增带宽。
另一方面，除了显存访问之外，还存在 kernel 启动开销和调度开销，
这些开销在 decode 阶段占比更高。
因此，显存带宽并不是 decode 阶段的唯一瓶颈，
性能提升会受到并行度和调度效率的共同限制。
```
