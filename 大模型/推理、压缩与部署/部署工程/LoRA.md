LoRA(Low-Rank Adaptation)  是一种**高效微调大语言模型（LLM）与视觉模型的轻量化技术**，
核心思想是通过引入**低秩矩阵**分解来模拟预训练模型权重的增量变化，仅训练少量低秩参数即可实现模型适配下游任务，大幅降低训练成本与显存占用。

## 1. 核心背景与痛点

在 LoRA 出现之前，大模型微调存在明显瓶颈：

1. 全参数微调

   需要训练模型所有权重（如 GPT-3 有 1750 亿参数），显存占用极高（通常需要多块高端 GPU），训练时间长、计算成本昂贵。

2. 冻结预训练权重 + 训练任务头

   仅训练少量任务特定参数，显存占用低，但模型适配能力弱，难以充分挖掘预训练模型的潜力，下游任务性能不佳

3. Adapter 等轻量化方法

   虽降低了参数量，但引入额外推理延迟，且结构相对复杂。

## 2. 核心原理

**1. 核心思想：低秩增量更新**

预训练模型的权重矩阵（以 Transformer 的注意力层权重 $W \in R^{d\times k}$）为例， $d$为输入维度， $k$ 为输出维度）通常具有低秩特性：
即权重的核心信息可以通过低秩矩阵来近似表示。

LoA 不直接更新预训练权重 $W$，而是为其引入一个**可训练的低秩增量矩阵**，最终的权重输出为：

 $W_{final}= W_{pretrained}$ + \trig W$

其中，低秩增量矩阵 $\trig W$由两个低秩矩阵相乘得到：

 $\trig W = A \dot B$

+ $A \in R^{d\time r}$：随机初始化的低秩矩阵（输入投影矩阵）

+ $B \in R^{r \times k}$：初始化为全 0 的低秩矩阵（输出投影矩阵）

+ $r$：秩（rank）是 LoRA 的核心超参数，通常设置为很小的值（如 8、16、32），远小于 $d$ 和 $k$


**2. 关键细节**

（1）参数规模对比

假设 $d=1024$，$k=1024$，$r=8$：

+ 原始权重 $W$ 的参数数量：$1024 \times 1024 = 1,048,576$

+ LoRA 增量矩阵 $\Delta W$ 的参数数量：$(1024 \times 8) + (8 \times 1024) = 16,384$

+ 仅占原始权重参数的 1.56%，大幅减少可训练参数规模

（2）训练与推理逻辑

+ 训练阶段：冻结预训练权重 \(W_{pretrained}\)，仅训练低秩矩阵 A 和 B，显存占用仅为全参数微调的几十分之一甚至百分之一；

+ 推理阶段：

  1. 可选方案 1（无延迟）

     将 $\Delta W = A \cdot B$ 直接累加到预训练权重 $W_{pretrained}$ 上，得到 $W_{final}$，推理时与原始模型完全一致，无额外计算开销。
 
  2. 可选方案 2（灵活切换）

     保留 $A$ 和 $B$，推理时动态叠加增量，支持快速切换不同任务的 LoRA 权重（即 “LoRA 权重切换”，无需重新加载整个模型）

  （3）核心应用层：Transformer 注意力层

  LoRA 通常不作用于模型所有层，而是优先作用于 Transformer 的自注意力层（Query/Key/Value 投影矩阵，尤其是 Query 和 Key 矩阵），这是因为注意力层决定了模型对上下文的建模能力，对下游任务的适配最为关键.

  + 可选择：仅对 $W_q$（Query 投影矩阵）和 $W_k$（Key 投影矩阵）添加 LoRA，进一步减少参数数量
 
  + 视觉模型（如 ViT）中，LoRA 通常作用于注意力层或全连接层。

  3. 超参数说明

 ## 3. 核心优势

 ## 4. 典型变体
