
模型蒸馏（knowledge Distillation, KD）是一种将大模型的“知识”压缩到小模型中的技术，目的是在尽量保持性能的前提下，显著降低模型规模、
推理延迟和计算成本。

## 1. 直观理解

让小模型模仿大模型的“思考方式”，而不只是模仿正确答案。

+ Teacher（教师模型）：大、准、慢

+ Student（学生模型）：小、快、部署友好

+ 蒸馏的关键

  学习 teacher 输出的“软概率分布”

## 2. 为什么蒸馏有效

### 2.1 硬标签 VS 软标签

+ 硬编码

```
猫 → [1, 0, 0, 0]
```

+ 软标签

```
猫 → [0.70, 0.20, 0.05, 0.05]
```

软标签包含了：

+ 类别之间的相似性

+ teacher 的不确定性

+ 暗含的高阶知识

## 3. 核心数学原理

### 3.1 Softmax + 温度参数（Temperature）

 $p_i=\frac{exp(z_i/T)}{\sum{j}{}{exp(z_j/T)}}$

+ T = 1：普通 softmax

+ T > 1: 分布更平滑（蒸馏常用）

经验值： $T = 2 ~ 5$

### 3.2 蒸馏损失函数（标准形式）

 $L=\alpha\cdot L_{CE}(y,p_s)+(1-\alpha)\cdot T^2\cdot L_{KL}(p_{t}^{T},p_{s}^{T})$

+ $p_{t}^{T}$

  teacher 的软输出

+ $p_{s}^{T}$

  student 的软输出

+ $CE$

  交叉熵（硬标签）

+ $KL$

  KL 散度

+ $\alpha$

  权重平衡

为什么乘 $T^2$：保持梯度尺度稳定

## 4. 完整训练流程

**Step 1：训练 Teacher**

+ 大模型

+ 性能优先

+ 可使用 ensemble / 数据增强

**Step 2：冻结 Teacher**

+ 推理模式

+ 不参与反向传播

**Step 3：训练 Student （蒸馏）**

```
输入 x
 ├─ Teacher → soft logits (T)
 └─ Student → logits
      ├─ CE(student, y)
      └─ KL(student_T, teacher_T)
```

## 5. 常见蒸馏方式分类

### 5.1 Logits 蒸馏（最经典）

+ 模仿输出概率

+ 简单、通用

适用：分类 / NLP / 多分类任务

### 5.2 特征蒸馏（Feature Distillation）

+ 对齐中间特征

+ 常用于 CNN / ViT

 $L_{feat}=||f_t - f_s||_{2}^{2}$

适用于：目标检测、分割

### 5.3 关系蒸馏（Relation KD）

+ 学样本之间的相似度结构

+ 不强制特征维度一致

代表：

+ RKD

+ CRD

### 5.4 自蒸馏（Self Distillation）

+ teacher = student（不同层 / 不同 epoch）

特点：

+ 无需大模型

+ 常用于 ViT、ResNet

### 5.5 在线蒸馏

+ teacher 与 student 同时训练

+ 如：Deep Mutual Learning

## 6. 蒸馏在不同领域的应用

### 6.1 NLP

+ BERT → TinyBERT / DistilBERT

+ 常蒸馏：

  + logits

  + attention map
  
  + hidden states

### 6.2 CV

+ ResNet-152 → ResNet-50

+ ViT → MobileViT

+ 常结合 feature distillation

### 6.3 大模型（LLM）

+ GPT-4 -> 7B / 3B

+ Sequence-level KD

+ Policy Distillation（RLHF 场景）

## 7. 实践经验 & 调参建议（非常重要）

### 7.1 超参数推荐

| 参数         | 建议      |
| ---------- | ------- |
| T          | 2–5     |
| α          | 0.1–0.5 |
| 蒸馏 loss 占比 | 30%–70% |

### 7.2 常见坑

❌ teacher 太弱 → 蒸馏无收益

❌ student 太小 → 容量不足

❌ 只用 KL，不用 CE → 容易偏移标签

❌ 数据分布不一致 → 蒸馏失败

## 8. 蒸馏 VS 其他压缩方法

| 方法   | 是否需 teacher | 优点  | 缺点    |
| ---- | ----------- | --- | ----- |
| 蒸馏   | ✅           | 性能好 | 训练复杂  |
| 剪枝   | ❌           | 快   | 易掉点   |
| 量化   | ❌           | 推理快 | 精度敏感  |
| LoRA | ❌           | 低成本 | 非极致压缩 |

最佳实践：

蒸馏 + 量化 + 剪枝 = 工业级方案
