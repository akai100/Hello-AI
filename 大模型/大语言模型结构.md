# 1. LLaMA 的模型结构


## 1.1 RMSNorm 归一化函数

为了使得模型训练过程更加稳定，GPT-2 相较于 GPT 就引入了前置归一化方法，将第一个层归一化移动到多头自注意力层之前，
第二个层归一化也移动到了全链接层之前，同时残差连接的位置也调整到了多头自注意力层与全连接层之后。

层归一化中也采用了 RMSNorm 归一化函数。计算公式如下：

 $RMS(a)=\sqrt(\frac{1}{n}\sum_{i=1}^{n}{a_{i}^{2}})$

## 1.2 SwiGLU 激活函数

SwiGLU 激活函数是 Shazeer 在文献中提出，并在 PaLM 等模中进行了广泛应用。 计算公式如下：

$FFN_{SwiGLU}(x, W, V, W_2)=SwiGLU(x, W, V)W_2$

$SwiGLU(x, W, V)=Swish_{ \belta}(xW)⊗xV$

## 1.3 旋转位置嵌入（RoPE）

