# 1. LLaMA 的模型结构


## 1.1 RMSNorm 归一化函数

为了使得模型训练过程更加稳定，GPT-2 相较于 GPT 就引入了前置归一化方法，将第一个层归一化移动到多头自注意力层之前，
第二个层归一化也移动到了全链接层之前，同时残差连接的位置也调整到了多头自注意力层与全连接层之后。

层归一化中也采用了 RMSNorm 归一化函数。计算公式如下：

<div align="center">

 $RMS(a)=\sqrt{(\frac{1}{n}\sum_{i=1}^{n}{a_{i}^{2}})}$
 
 $\~{a}$

 </div>

 此外，RMSNorm 还可以引入可学习的缩放因子 $g_i$ 和偏移参数 $b_i$，从而得到 

 <div align="center">
 $a_i=\frac{a_i}{RMS(a)}g_i+b_i$
 </div>

## 1.2 SwiGLU 激活函数

SwiGLU 激活函数是 Shazeer 在文献中提出，并在 PaLM 等模中进行了广泛应用。 计算公式如下：

<div align="center">

$FFN_{SwiGLU}(x, W, V, W_2)=SwiGLU(x, W, V)W_2$

$SwiGLU(x, W, V)=Swish_{ \beta}(xW)⊗xV$

$Swish_\beta(x)=x$

</div>

## 1.3 旋转位置嵌入（RoPE）

