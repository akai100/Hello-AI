对齐微调（Alignment Fine-tuning）是大模型训练流程中至关重要的一环，**核心目标**是让预训练大模型的输出**对齐人类的需求和偏好**—— 解决预训练模型 “能力强但不听话” 的问题，
比如预训练模型可能生成语法正确但无意义、有害、不符合指令、或与人类价值观相悖的内容，而对齐微调就是为了修复这些问题。

## 核心定义

### 本质

在预训练大模型（Foundation Model）的基础上，通过有针对性的微调数据和训练策略，让模型的输出满足人类的三大核心需求：

+ 指令遵循

  准确理解并执行人类的自然语言指令（如“写一篇关于环保的短文”总结这段文字的核心观点）；

+ 偏好对齐

  生成符合人类审美、价值观和使用习惯的内容（如更流畅、更简介、更正面的表达）；

+ 安全对齐

  避免生成有害、歧视、暴力、虚假的内容（如拒绝回答如何制作炸弹，纠正错误的事实陈述）；


### 与传统微调的区别

| 维度	| 传统微调（Task-specific Fine-tuning）	| 对齐微调（Alignment Fine-tuning） |
|-------|---------------------------------------|-----------------------------------|
| 目标	| 让模型适配特定下游任务（如分类、翻译、问答） |	让模型适配人类的通用需求（指令、偏好、安全） |
| 数据	| 任务相关的标注数据（如分类数据集、问答对）	| 指令数据、人类偏好排序数据、安全对齐数据 |
| 训练策略	| 普通的监督学习（MLE）| 	监督微调（SFT）+ 奖励模型训练（RM）+ 强化学习（RLHF） |
| 适用场景	| 特定任务的模型部署（如企业内部的文本分类模型）	| 通用大模型的产品化（如 ChatGPT、文心一言、Llama 2 Chat） |

## 核心技术与方法

对齐微调的**标准流程**是 **「监督微调（SFT）→ 奖励模型训练（RM）→ 强化学习微调（RLHF）」**，这也是 OpenAI、Anthropic 等公司的核心技术路线。部分轻量化方案会省略强化学习步骤，仅用 SFT 实现基础的对齐。

### 监督微调

+ 核心逻辑

  用**高质量的指令 - 响应对（Instruction-Response Pairs）** 对预训练模型进行监督学习，让模型学会基本的指令遵循能力。

+ 关键步骤

1. 构建 SFT 数据集：收集大量人类编写的指令和对应的理想响应（如 Alpaca 的 52K 指令数据、Self-Instruct 的自生成指令数据）；
 
2. 训练策略：使用**因果语言模型（CLM）的损失函数**，让模型学习从指令生成对应的响应，训练时冻结预训练模型的底层参数（可选），仅微调顶层参数，减少计算量；
 
3. **作用**：为模型打下基础的指令遵循能力，是对齐微调的第一步，也是必不可少的一步。

### 奖励模型训练

+ 核心逻辑

  训练一个奖励模型，让它能够根据**人类的偏好**，为模型的不同输出打分（分数越高，越符合人类偏好）。

+ 关键步骤

1. 构建偏好数据集：对同一个指令，让模型生成多个不同的响应，然后让人类标注这些响应的**偏好排序**（如 A > B > C）

2. 训练策略：将偏好排序数据转换为**成对比较数据**，训练奖励模型对更优的响应给出更高的分数，损失函数通常使用**排序损失**（Ranking Loss）。

3. 作用：将人类的偏好量化为可计算的奖励分数，为后续的强化学习提供优化目标。

### 强化学习微调

+ 核心逻辑

  以奖励模型的分数为奖励信号，使用强化学习算法（如 PPO, Proximal Policy Optimization）对 SFT 模型进行微调，让模型学会生成**奖励分数更高**的内容

+ 关键步骤

1. 初始化策略模型：以 SFT 模型作为强化学习的初始策略模型

2. 强化学习训练：模型生成响应，奖励模型给出分数，通过 PPO 算法更新模型参数，让模型在**保持指令遵循能力的同时**，生成更符合人类偏好的内容；

3. 关键技术：**KL 散度约束**（KL Regularization），防止模型在强化学习过程中偏离 SFT 模型的分布过远，导致生成内容质量下降；

4. 作用：进一步提升模型的偏好对齐能力，是对齐微调的核心步骤，也是让模型 “更听话” 的关键。

## 轻量级对齐微调方案

工业界的 RLHF 流程计算成本极高（需要训练三个模型：SFT 模型、奖励模型、强化学习模型），对于资源有限的场景，通常采用以下轻量化方案：

1. 仅用 SFT：用高质量的指令 - 响应对进行监督微调，实现基础的指令遵循能力，如 Alpaca、Vicuna 等开源模型。

2. SFT + 人工规则过滤：在 SFT 的基础上，加入人工编写的规则和过滤模块，实现基础的安全对齐，如 Llama 2 的安全过滤机制。

3. 蒸馏对齐（Alignment Distillation）：用大模型的对齐能力蒸馏小模型，让小模型在保持轻量化的同时，拥有大模型的对齐能力，如 DistilGPT2、TinyLlama 等

## 工业界实践关键点

1. **数据质量是核心**：对齐微调的效果**高度依赖于数据的质量**，而非数据的数量。高质量的指令 - 响应对需要满足：指令多样、响应准确、语言流畅、符合人类偏好;

2. 训练策略的优化

   + 参数高效微调（PEFT）

     在对齐微调中，通常使用 LoRA、Prefix Tuning 等 PEFT 方法，仅微调模型的部分参数，大幅减少计算量和显存占用，适合在消费级 GPU 上进行实践

   + 学习率调度

     使用较小的学习率（如 1e-5 ~ 1e-4），避免模型过拟合到对齐数据，同时保持预训练模型的通用能力。

3. 评估指标的设计

   对齐微调的评估比传统微调更复杂，需要从指令遵循、偏好对齐、安全对齐三个维度进行评估，常用的评估方法包括：

   + 人工评估

     让人类标注员对模型的输出进行打分，是最准确的评估方法

   + 自动评估

     使用预训练的评估模型（如 GPT-4）对模型的输出进行打分，或使用 BLEU、ROUGE、Perplexity 等指标进行辅助评估
