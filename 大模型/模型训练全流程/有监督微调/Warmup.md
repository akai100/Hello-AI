**Warmup** 是指在训练深度学习模型时，**在初期阶段逐步增加学习率**，而不是从一开始就使用预设的较大学习率。
这一策略通常用于防止训练初期因为学习率过大导致梯度爆炸或不稳定，从而有助于模型更好地收敛。

## 为什么使用 Warmup？

在深度学习中，**学习率**是影响训练稳定性和收敛速度的重要超参数。如果一开始学习率设置得过大，可能会导致梯度过大，进而使模型无法稳定收敛，
甚至出现 NaN 或 Inf 等错误。因此，Warmup 通过在初期阶段逐步增加学习率来缓解这个问题，使得模型在训练的早期阶段逐渐适应较大的学习率。

## Warmup 的工作原理

**1. 开始时学习率小**：初始时使用一个较小的学习率，防止模型开始训练时过大步长，导致梯度爆炸或不稳定。

**2. 逐步增加学习率**：在前几步中，学习率逐步线性增加或按某种策略增加，直到达到预设的最大值或目标学习率。

**3. 进入正常训练阶段**：一旦达到目标学习率，学习率就保持不变或按学习率衰减策略减小。

## Warmup 的常见策略

### 1. 线性 Warmup（Linear Warmup）：

+ 线性增加学习率是最常见的 Warmup 策略。

+ 在训练的前 $n$ 步中，学习率从一个小值线性增加到预定的目标学习率。

公式：

$$lr_{current}=lr_{start}+(\frac{lr_{target}-lr_{start}}{warmup\\_steps})\times step$$

其中，$lr_start$ 是初始学习率，$lr_target$ 是目标学习率，$warmup_steps$ 是 Warmup 阶段的步数，$step$ 是当前的训练步数。

### 2. 指数 Warmup（Exponential Warmup）：

学习率以指数方式增长，从较小的值开始，逐渐增加到目标学习率。这种方式增长速度更快，适用于某些特定任务。

公式：

$$lr_{current}=lr_{start}\times (\frac{lr_{target}}{lr_start})^{\frac{step}{warmup\\_steps}}$$

### 3. 多阶段 Warmup：

+ 在训练过程中使用多个阶段的 Warmup，每个阶段可能使用不同的增长策略（例如，前期线性增长，后期指数增长）。

### 4. Cosine Warmup：

+ 学习率按余弦函数逐渐增加，形态较平滑，适用于想要在训练初期有更平缓学习率增长的情况。

公式：

$$lr_{current}=lr_{start}+0.5\times (lr_{target}-lr_{start})\times (1+cos(\frac{\pi \times step}{warmup_steps}))$$

## Warmup 与学习率衰减（Learning Rate Decay）

Warmup 通常与学习率衰减策略（如 Step Decay、Cosine Annealing、Exponential Decay 等）结合使用。Warmup 阶段结束后，学习率开始按照预设的衰减策略逐步减小。

例如：

+ 在 Warmup 阶段使用线性增长的策略，训练的前 10% 步骤逐步增加学习率；

+ 然后进入 学习率衰减阶段，开始应用例如 Step Decay 或 Cosine Annealing 的学习率衰减策略。

## Warmup 在实践中的作用

1. 防止梯度爆炸：在训练初期使用较小的学习率，有助于防止梯度爆炸，尤其是在训练大模型时。

2. 加速收敛：通过逐步增加学习率，模型可以在早期阶段避免过度振荡，减少初期训练的不稳定性，从而加速训练的后期收敛。

3. 提高稳定性：Warmup 使得优化过程更加平滑，避免训练初期剧烈的梯度变化，从而帮助模型更稳定地收敛。

## Warmup 的典型应用场景

+ 大规模预训练模型：在训练大规模的预训练模型（如 GPT、BERT、T5 等）时，使用 Warmup 可以避免训练初期的梯度爆炸，确保模型稳定收敛。

+ 较大 batch size 训练：当使用较大的 batch size 时，初期阶段较大的学习率可能会导致训练不稳定，使用 Warmup 可以帮助模型逐步适应。

+ Fine-tuning：在进行模型微调时，尤其是当训练集较小或类别不均衡时，Warmup 可以有效防止模型在训练初期对某些类别过拟合。
