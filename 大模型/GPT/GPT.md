
OpenAI 公司在 2018 年提出的生成式预训练语言模型（Generative Pre-Training, GPT）是典型的生成式预训练语言模型之一。

# 1. 无监督预训练

GPT 采用生成式预训练方法，单向意味着模型只能从左到右或从右到左对文本序列建模，所采用的 Transformer 结构和解码策略保证了
输入文本每个位置只能依赖过去时刻的信息。

**步骤 一：**

给定文本序列 $w = w_1w_2...w_n,$ GPT 首先在输入层中将其映射为稠密的向量：

$v_i=v_{i}^{t}+v_{i}^{p}$

其中，
+ $v_{i}^{t}$ 是词 $w_{i}$ 的词向量，

+ $v_{i}^{p}$ 是词 $w_i$ 的位置向量，

+ $v_i$ 为第 $i$ 个位置的单词经过模型输入层（第 0 层）后的输出；

GPT 模型的输入层与前文中介绍的神经网络语言模型的不同之处在于其需要添加位置向量，这是Transformer 结构自身无法感知位置导致的，因此需要来自输入层的额外位置信息

**步骤 二：**

经过输入层编码，模型得到表示向量序列 $v=v_1...v_n$，随后将 $v$ 送入模型编码层。

编码层由 $L$ 个 Transformer 模块组成，在自注意力机制的作用下，每一层的每个表示向量都会包含之前位置表示向量的信息，使每个表示向量都具备丰富的上下文信息，
并且经过多层编码后，GPT 能得到每个单词层次化的组合式表示，其计算过程表示如下：

$h^{(L)}=Transformer-Block^{(L)}(h^{(0)})$

其中：

+ $h^{(L)}\inR^{d * n}$ 表示第 $L$ 层的表示向量序列；

+ $n$ 为序列长度；

+ $d$ 为模型隐藏层维度；

+ $L$ 为模型总层数；

GPT 模型的输出层基于最后一层的表示 $h^{(L)}$ ，预测每个位置上的条件概率，其计算过程可以表示为：

$P(w_i|w_1,...,w_{i-1})=Softmax(W^eh_{i}^{(L)}+b^{out})$

其中，

+ $W^e \in R^{|V| * d}$ 为 词向量矩阵；

+ $|V|$ 为词表大小；


**单向语言模型** 是按照阅读顺序输入文本序列 $w$，用常规语言模型目标优化 $w$ 的最大似然估计，使只能根据输入历史序列对当前词能够做出准确的预测：


# 2. 有监督下游任务微调

通过无监督语言模型预训练，使得GPT 模型具备了一定的通用语义表示能力。下游任务微调（Downstream Task Fine-tuning）的目的是在通用语义表示基础上，根据下游任务的特性进行适配。

下游任务通常需要利用有标注数据集进行训练，数据集合使用 $D$ 进行表示，每个样例由输入长度为 $n$ 的文本序列 $x = x1x2...xn$ 和对应的标签 $y$ 构成。
