
**背景：**

由于大语言模型参数量十分庞大，当将其应用到下游任务时，微调全部参数需要相当高的算力。

**目的：**

在仅训练少量参数使模型适应到下游任务。

## 1. LoRA

研究表明，语言模型针对特定任务微调之后，权重矩阵通常具有很低的本征秩（Intrinsic Rank）。

研究人员认为参数更新量即便投影到较小的子空间中，也不会影响学习的有效性。

因此，提出固定预训练模型参数不变，在原本权重矩阵旁路添加低秩矩阵的乘积作为可训练参数，用以模拟参数的变化量。

具体来说，假设预训练权重 $W_0 \in R^{d * k}$，可训练参数为 $\trig W = BA$，

其中 $B \in R^{d*r}$， $A \in R^{r * d}$。

初始化时 $A$ 通过高斯函数初始化，矩阵 $B$ 为零初始化，使得训练开始之前旁路对原模型不造成影响。

对于该权重的输入 $x$ 来说：

 $h=W_0x+ \trig Wx =W_0x + BAx $

LoRA(Low-Rank Adaptation of Large Language Models),

其他高效微调方法，如微调适配器（Adapter）和前缀微调（Prefix Tunning）。
