
**误差逆传播**（Backpropagation，简称反向传播）是深度学习中训练神经网络的核心算法，其本质是利用**链式法则**，将输出层的误差反向传递到每一层，计算每个参数的梯度，再通过梯度下降更新参数。

下面以一个例子来介绍反向传播的过程：

（1）神经网络结构

输入层：2个 $(x_{1}, x_{2})$ 神经元


隐藏层：2个 $(h_{1}, h_{2})$，激活函数：Sigmoid 权重： $W_{1}(2 \times 2)、b_{1}(2 \times 1)$

输出层：1 个 $\hat{y}$   激活函数：Sigmoid  权重： $W_{2}(2 \times 1)、b_{2} (1 \times 1)$

+ 输入： $X = [x_{1}, x_{2}]$

+ 隐藏层加权输入： $Z_{1} = X \dot W_{1} + b_{1}$

+ 隐藏层输出： $H = \sigma (Z_{1})$

+ 输出层加权输入： $Z_{2}=H \dot W_{2} + b_{2}$

+ 预测输出： $\hat{y}= \sigma (Z_{2})$
