# 1. 梯度消失

梯度消失是深度学习中最经典的挑战之一，指在深度神经网络训练过程中，反向传播时梯度从输出层向输入层传递时逐渐衰减，最终导致浅层（靠近输入层）的权重几乎无法更新，模型训练失败或性能不佳。

# 2. 核心原理

梯度消失的根源在于反向传播算法的链式法则。

假设网络有$L$层，第 $l$ 层的激活函数为 $\(g_l\)$ ，权重为 $\(W^l\)$，则第 $l$ 层权重的梯度 $\(\frac{\partial Loss}{\partial W^l}\)$ 可通过链式法则展开为：
