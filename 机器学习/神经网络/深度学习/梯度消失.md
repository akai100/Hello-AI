# 1. 梯度消失

梯度消失是深度学习中最经典的挑战之一，指在深度神经网络训练过程中，反向传播时梯度从输出层向输入层传递时逐渐衰减，最终导致浅层（靠近输入层）的权重几乎无法更新，模型训练失败或性能不佳。

# 2. 核心原理

梯度消失的根源在于反向传播算法的链式法则。

假设网络有 $L$层，第 $l$ 层的激活函数为 $\(g_l\)$ ，权重为 $\(W^l\)$，则第 $l$ 层权重的梯度 $\(\frac{\partial Loss}{\partial W^l}\)$ 可通过链式法则展开为：

$\beta$
 
​
关键问题在于：当激活函数的导数 $\(g'_k(z^k) < 1\)$ 时，多层乘积会导致梯度指数级衰减。

# 3. 梯度消失的主要原因

1. 激活函数选择不当

2. 网络过深

3. 权重初始化不当


# 4. 梯度消失的危害

+ 浅层权重无法更新

  输入层附近的权重梯度接近 0，导致这些层的参数几乎不更新，网络无法学习到有效的底层特征（如边缘、纹理）；

+ 模型训练停滞

  损失函数下降缓慢或不再下降，模型无法收敛到最优解

+ 深层网络无法训练

  限制了网络深度的提升，无法利用深层网络的强大拟合能力

# 5. 解决梯度消失的方法


1. 更换激活函数

   使用导数范围更大、不易饱和的激活函数，如： RELU（Rectified Linear Unit）

2. 批量归一化

3. 残差连接

4. 适当额权重初始化

5. 梯度裁剪

6. 使用更先进的网络架构

# 6. 梯度消失与梯度爆炸的关系

