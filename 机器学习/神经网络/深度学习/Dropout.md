# 1. Dropout

Dropout 是深度学习中一种常用的正则化技术，核心目标是防止模型过拟合，提升模型的泛化能力，尤其适用于全连接层较多的深度神经网络。

# 2. 原理

过拟合的本质是模型过度依赖训练数据中的局部特征或噪声，导致在训练集上表现极好，但在测试集上表现糟糕。

Dropout 的解决思路：在训练过程中随机 “丢弃” 一部分神经元，让这些神经元不参与前向传播和反向传播，从而打破神经元之间的协同依赖关系，迫使模型学习更鲁棒、更通用的特征。

具体过程分为 训练阶段 和 测试阶段，两者逻辑不同：

1. 训练阶段

+ 设定一个丢弃概率 p（超参数，常用值为 0.2~0.5），表示每个神经元被随机丢弃的概率；

+ 对每一层的神经元，按照概率 p 随机选中部分神经元，将其输出值置为 0，使其暂时失效；

+ 为了保证训练和测试阶段的输出分布一致，需要对未被丢弃的神经元输出进行 缩放：乘以  $\frac{1}{1−p} $，补偿被丢弃神经元的贡献

   
3. 测试阶段

+ 不进行任何神经元丢弃，所有神经元都参与计算；

+ 无需缩放操作，因为训练阶段已经通过缩放保证了期望输出的一致性；


# 3. 关键超参数

+ 丢弃概率 $p$

  + 是Dropout 最核心的超参数，需要根据任务调整；

    + $p$ 过小：丢弃的神经元少，正则化效果弱，仍可能过拟合；
   
    + $p$ 过大：丢弃的神经元多，模型学习不到足够特征，可能欠拟合；

经验值：全连接层常用 $p=0.5$，卷积层一般不用或用较小的 $p$（如 0.2），因为卷积层的参数共享本身就有一定正则化效果。

# 4. 变体

1. Dropout2D/Dropout3D

针对卷积层的特征图设计，不是随机丢弃单个神经元，而是随机丢弃整个特征通道（即对特征图的某个通道全部置 0），避免破坏卷积特征的空间结构，常用于 CNN 中；

2. Alpha Dropout

  适用于自归一化网络（如 SELU 激活函数的网络），在丢弃神经元的同时，保证输出的均值和方差不变，无需额外缩放；

3. Spatial Dropout

   对特征图的空间位置进行随机丢弃，即对特征图的某个空间位置（如 $H×W$ 上的某个像素）的所有通道置 0，适用于需要保留通道间相关性的任务
