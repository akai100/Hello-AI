Adam（Adaptive Moment Estimation，自适应矩估计）是一种在深度学习中**最常用、最稳定**的优化算法之一，结合了 Momentum 和 RMSProp 的思想。

## 1. 解决问题

目标是最小化损失函数：

 $min_\theta f(\theta)$

问题在于：

+ 梯度震荡（方向来回摆）

+ 学习率难以统一（不同参数梯度尺度差异大）

+ 纯 SGD 收敛慢、容易卡住

Adam 目标：

为 **每一个参数** 自动调整学习率，并且利用历史梯度信息加速收敛。

## 2. 核心思想

Adam = Momentum(一阶动量) + RMSProp（二阶动量）+ 偏差修正

+ 一阶矩：梯度的**指数加权平均（方向）**

+ 二阶矩：梯度平方的**指数加权平均（尺度）*

+ 偏差修正：解决初期估计偏小的问题

## 3. 数学公式详解

设：

+ 参数： $\theta$

+ 当前梯度： $g_t=∇_\theta f(\theta_t)$

### 3.1 一阶矩估计

 $m_t=\beta_1 m_{t-1} + (1-\beta_1)g_t$

含义：

+ 梯度的“滑动平均”

+ 控制更新方向的稳定性

### 3.2 二阶矩估计（RMSProp）

 $v_t=\beta_2v_{t-1}+(1-\beta_2)g_{t}^2$

含义：

+ 梯度平方的滑动平均

+ 控制每个参数的学习率大小

### 3.3 偏差修正

由于 $m_0=0,v_0=0$，前期会偏小：

 $\hat{m_t}=\frac{m_t}{1-\beta_{1}^{t}}, \hat{v_t}=\frac{v_t}{1-\beta_{2}^{t}}$

### 3.4 参数更新方式

 $\theta_{t+1}=\theta_t-\alpha \frac{\hat{m_t}}{\sqrt{\hat{v_t}}+ε}$

+ 方向：由 $\hat{m_t}$决定

+ 步长：由 $\sqrt{\hat{v_t}}$自动缩放

+ 梯度大 -> 步子小

+ 梯度小 -> 步子大

