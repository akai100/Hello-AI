
## 解决问题

解决 Adagrad 的致命缺陷

**👉 Adagrad 的问题：**

+ 累积平方梯度

+ 学习率 单调下降

+ 训练后期 几乎不动

## 一句话直觉

别记一辈子梯度，只记“最近一段时间”

RMSprop 的核心就是：

**“让优化器会遗忘”**

## 从 Adagrad 到 RMSprop 的直觉演化

### 1️⃣ Adagrad 在做什么？

$$G_t=\sum_{\tau=1}^{t}{g_{\tau}^{2}}$$

问题：

+ 远古梯度影响永远存在

+ 即时现在梯度很小，步子也回不来

### 2️⃣ RMSprop 的改进

**用指数滑动平均代替累积**

$$v_t=\beta v_{t-1}+(1-\beta)g_{t}^2$$

📌 关键点：

+ $\beta \in [0.9, 0.99]$

+ 越久远的梯度，权重越小

👉 这就是“遗忘机制”

## RMSprop 的更新公式

$$\theta_{t+1}=\theta_t - \frac{\eta}{\sqrt{v_t} + \epsilon} \cdot g_t$$

**每个参数的有效学习率**：

$$lr_i=\frac{\eta}{\sqrt{v_{t,i}}}$$

📌 梯度大 → 步子小

📌 梯度小 → 步子大

## 几何直觉（非常重要）

**原始参数空间**

```
   steep
     ↑
     |
     |
     +------→ flat
```

**RMSprop 做了什么？**

+ 压缩陡峭方向

+ 拉伸平缓方向

```
真实地形（椭圆） → RMSprop 看到的地形（接近圆）
```

👉 在“更圆”的空间里下降更稳定

## RMSprop vs Adagrad（核心区别）

| 维度   | Adagrad | RMSprop  |
| ---- | ------- | -------- |
| 二阶信息 | 累积平方梯度  | **指数平均** |
| 是否遗忘 | ❌       | ✅        |
| 学习率  | 单调下降    | 稳定       |
| 长训练  | ❌       | ✅        |

📌 一句话：

RMSprop = Adagrad + 忘记机制

## RMSprop 和 Adam 的关系

**Adam = RMSprop + Momentum**

| 组件      | RMSprop | Adam |
| ------- | ------- | ---- |
| 二阶矩     | ✅       | ✅    |
| 一阶矩（方向） | ❌       | ✅    |
| 偏差修正    | ❌       | ✅    |

Adam 在 RMSprop 基础上加了：

+ 梯度的滑动平均（Momentum）

+ 偏差修正

## RMSprop 的优缺点

### ✅ 优点

+ 比 Adagrad 稳定

+ 对学习率不太敏感

+ 非常适合 非平稳目标函数

+ 训练 RNN / RL 常用

### ❌ 缺点

+ 没有 Momentum → 方向信息不足

+ 收敛速度通常慢于 Adam

+ 理论性质不如 SGD 清晰

## 典型应用场景

### 📌 RMSprop 特别适合：

+ RNN

+ 强非平稳问题

+ 强震荡梯度

+ 强尺度不一致问题

## 📌 不太适合：

+ 大模型长期训练（AdamW 更好）

+ 追求最终极致泛化

## 超参数怎么选？

| 参数 | 含义   | 常用值  |
| -- | ---- | ---- |
| η  | 学习率  | 1e-3 |
| β  | 二阶动量 | 0.9  |
| ε  | 数值稳定 | 1e-8 |

👉 一般 不用动 β

### 面试一句话版本（强烈推荐）

RMSprop 通过对梯度平方进行指数加权平均，避免了 Adagrad 中学习率单调衰减的问题，为每个参数自适应调整学习率，使优化过程在非平稳和高曲率方向上更加稳定。

##
