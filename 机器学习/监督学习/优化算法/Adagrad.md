
## 1. 解决问题

**核心问题：**

不同参数的梯度尺度差异巨大，**一个统一学习率不合理**

尤其常见于：

+ 稀疏特征（NLP、Embedding、One-hot）

+ 有的参数经常更新，有的很少更新

## 2. 思想

更新得越频繁的参数，步子越走越小

很少更新的参数，步子始终较大

👉 Adagrad 自动“惩罚”常出现梯度的维度

## 3. 从 SGD 到 Adagrad 的直觉推导

### 3.1 SGD 的问题

 $\theta_{t+1}=\theta_t-\eta g_t$

+ 每个参数用同一个学习率

+ 无视历史信息

+ 稀疏特征学得很慢

### 3.2 Adagrad的关键思想

**统计每个参数“历史上走了多少步”**

如果一个参数：

+ 梯度经常出现 → 已经学很多了 → 步子该小

+ 梯度很少出现 → 还没学够 → 步子该大

## 4. 数学公式

### 4.1 累积平方梯度

+ 参数： $\theta$

+ 学习率： $\eta$

+ 累积梯度平方： $G_0 = 0$

假设第 $t$步的梯度是： $g_t=∇_\theta L(\theta_t)$

（1）累积平方梯度

 $G_t=G_{t-1}+g_{t}^2$

（2） 参数更新

 $\theta_{t+1}=\theta_t - \sqrt{\eta}{\sqrt{G_t}+\eps} \cdot g_t$

拆开看，第 $i$个参数：

 $\theta_{i,t+1}=\theta_{i,t}-\frac{\eta}{\sqrt{\sum_{j=1}^{t}{g_{i,t}^2}}+\eps} \cdot g_{i,t}$


## Adagrad 的优点

✅ 1. 对稀疏特征非常友好

+ 低频参数 → 累积梯度小 → 学习率大

+ 高频参数 → 学习率自动衰减

✅ 2. 几乎不用调学习率

+ 一个较大的初始 $\eta$

+ 后面会自动变小

✅ 3. 理论上有收敛保证

+ 凸优化场景下有严格分析

## 致命缺点

**❌ 学习率单调下降，且不可逆**


因为：

 $G_t=\sum_{t}{}{g_{t}^2}$

+ 只增不减

+ $\sqrt{G_t}$越来越大

+ 学习率 → 0

**👉 训练后期基本“学不动”**

这也是为什么：

**Adagrad 几乎不用在深度学习主流模型里**
