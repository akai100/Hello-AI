
## 1. 解决问题

**核心问题：**

不同参数的梯度尺度差异巨大，**一个统一学习率不合理**

尤其常见于：

+ 稀疏特征（NLP、Embedding、One-hot）

+ 有的参数经常更新，有的很少更新

## 2. 思想

更新得越频繁的参数，步子越走越小

很少更新的参数，步子始终较大

👉 Adagrad 自动“惩罚”常出现梯度的维度

## 3. 从 SGD 到 Adagrad 的直觉推导

### 3.1 SGD 的问题

 $\theta_{t+1}=\theta_t-\eta g_t$

+ 每个参数用同一个学习率

+ 无视历史信息

+ 稀疏特征学得很慢

### 3.2 Adagrad的关键思想

**统计每个参数“历史上走了多少步”**

如果一个参数：

+ 梯度经常出现 → 已经学很多了 → 步子该小

+ 梯度很少出现 → 还没学够 → 步子该大

## 4. 数学公式

### 4.1 累积平方梯度

对每个参数 $i$:

$G_{t,i}=$
